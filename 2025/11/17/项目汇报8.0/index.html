<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>项目汇报8.0 | xxxkkw的妙妙屋</title><meta name="author" content="xxxkkw"><meta name="copyright" content="xxxkkw"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Rebuttal为应对审稿人提出的一些问题以及需要补充的实验，这里总结一下所跑的实验结果以及一些注意点 PPG数据分类结果审稿人1与4均提及了我们只做了PSG数据的事情，所以特意找来了一个PPG的数据集，且在最近的顶会上有过相关工作的，所以也跑了一些实验，数据集叫WESAD，这是一个四分类的情绪数据集，结果如下     model ACC AUROC AUPRC F1 Kappa     REBA"><meta property="og:type" content="article"><meta property="og:title" content="项目汇报8.0"><meta property="og:url" content="http://example.com/2025/11/17/%E9%A1%B9%E7%9B%AE%E6%B1%87%E6%8A%A58.0/index.html"><meta property="og:site_name" content="xxxkkw的妙妙屋"><meta property="og:description" content="Rebuttal为应对审稿人提出的一些问题以及需要补充的实验，这里总结一下所跑的实验结果以及一些注意点 PPG数据分类结果审稿人1与4均提及了我们只做了PSG数据的事情，所以特意找来了一个PPG的数据集，且在最近的顶会上有过相关工作的，所以也跑了一些实验，数据集叫WESAD，这是一个四分类的情绪数据集，结果如下     model ACC AUROC AUPRC F1 Kappa     REBA"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/img/head.jpg"><meta property="article:published_time" content="2025-11-17T07:00:00.000Z"><meta property="article:modified_time" content="2025-11-19T14:28:58.841Z"><meta property="article:author" content="xxxkkw"><meta property="article:tag" content="项目"><meta property="article:tag" content="python"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://example.com/img/head.jpg"><link rel="shortcut icon" href="/img/wall.jpg"><link rel="canonical" href="http://example.com/2025/11/17/%E9%A1%B9%E7%9B%AE%E6%B1%87%E6%8A%A58.0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{const e={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:e,getScript:(e,t={})=>new Promise((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach(([e,t])=>n.setAttribute(e,t)),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),getCSS:(e,t)=>new Promise((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),addGlobalFn:(e,t,o=!1,a=window)=>{if(e.startsWith("pjax"))return;const n=a.globalFn||{};n[e]=n[e]||{},o&&n[e][o]||(n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n)}};const t=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},o=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","ffffff")};btf.activateDarkMode=t,btf.activateLightMode=o;const a=e.get("theme");"dark"===a?t():"light"===a&&o();const n=e.get("aside-status");void 0!==n&&document.documentElement.classList.toggle("hide-aside","hide"===n);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:void 0,noticeOutdate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!0},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"项目汇报8.0",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2025-11-19 22:28:58"}</script><link rel="stylesheet" href="/styles/main.css"><head><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.3.0"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",o)})()</script><div id="web_bg" style="background-color:#efefef"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/head.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">35</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 文章</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(/img/wall.jpg)"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">xxxkkw的妙妙屋</span></a><a class="nav-page-title" href="/"><span class="site-name">项目汇报8.0</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 文章</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">项目汇报8.0</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-11-17T07:00:00.000Z" title="发表于 2025-11-17 15:00:00">2025-11-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-11-19T14:28:58.841Z" title="更新于 2025-11-19 22:28:58">2025-11-19</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2025/11/17/%E9%A1%B9%E7%9B%AE%E6%B1%87%E6%8A%A58.0/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2025/11/17/%E9%A1%B9%E7%9B%AE%E6%B1%87%E6%8A%A58.0/" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Rebuttal"><a href="#Rebuttal" class="headerlink" title="Rebuttal"></a>Rebuttal</h1><p>为应对审稿人提出的一些问题以及需要补充的实验，这里总结一下所跑的实验结果以及一些注意点</p><h2 id="PPG数据分类结果"><a href="#PPG数据分类结果" class="headerlink" title="PPG数据分类结果"></a>PPG数据分类结果</h2><p>审稿人1与4均提及了我们只做了PSG数据的事情，所以特意找来了一个PPG的数据集，且在最近的顶会上有过相关工作的，所以也跑了一些实验，数据集叫<code>WESAD</code>，这是一个四分类的情绪数据集，结果如下</p><div class="table-container"><table><thead><tr><th>model</th><th>ACC</th><th>AUROC</th><th>AUPRC</th><th>F1</th><th>Kappa</th></tr></thead><tbody><tr><td>REBAR (ICLR 2024)</td><td>0.418</td><td>0.698</td><td>0.446</td><td></td><td></td></tr><tr><td>cVAN</td><td>0.6914</td><td>0.8550</td><td>0.7104</td><td>0.6641</td><td>0.5716</td></tr><tr><td>Resnet</td><td>0.713</td><td>0.877</td><td>0.746</td><td>0.682</td><td>0.593</td></tr><tr><td>HuBERT (NeurIPS 2025)</td><td>0.775</td><td>0.82</td><td></td><td></td><td></td></tr><tr><td>S3Net</td><td>0.853</td><td>0.911</td><td>0.833</td><td>0.819</td><td>0.787</td></tr></tbody></table></div><p>另外需要说明，PPG测试模型删除了专家以及门控损失，相当于只有两层骨干结构，因为当前结构用不上我们为睡眠准备的专家以及门控损失，所以直接将门控的原本权重的输出换成了分类头，尽可能小的改动结构。</p><h2 id="对齐的各种改版"><a href="#对齐的各种改版" class="headerlink" title="对齐的各种改版"></a>对齐的各种改版</h2><ul><li>将我们的对齐直接替换成cVAN对齐</li><li>将对齐中的softmax加权替换成learnable pooling</li><li>直接展平，忽略原始时间前后文关系，加更强力的位置编码（这里需要额外去对比说明，我们保证了时间的上下文关系故更好）</li><li>不平方不加权</li></ul><div class="table-container"><table><thead><tr><th>version</th><th>ACC</th><th>F1</th><th>Kappa</th></tr></thead><tbody><tr><td>cVAN-ALN</td><td>0.858</td><td>0.820</td><td>0.812</td></tr><tr><td>pooling-ALN</td><td>0.861</td><td>0.831</td><td>0.816</td></tr><tr><td>flat-ALN</td><td>0.847</td><td>0.813</td><td>0.790</td></tr><tr><td>no-weight-ALN</td><td>0.860</td><td>0.825</td><td>0.811</td></tr></tbody></table></div><h2 id="参数量、计算复杂度与时间"><a href="#参数量、计算复杂度与时间" class="headerlink" title="参数量、计算复杂度与时间"></a>参数量、计算复杂度与时间</h2><p>这里采用Params(Million) GFlops两个参数作为指标，同时添加模型平均训练时间以及平均推理时间，共四个指标，均测试于batch size = 32,跑1000次求平均</p><div class="table-container"><table><thead><tr><th>model</th><th>Params(Million)</th><th>GFlops</th><th>train(ms)</th><th>infer(ms)</th></tr></thead><tbody><tr><td>MVF</td><td>39.51</td><td>11.06</td><td></td><td></td></tr><tr><td>cVAN</td><td>7.58</td><td>0.47</td><td>157.01</td><td>66.72</td></tr><tr><td>S3Net</td><td>6.49</td><td>2.70</td><td>75.69</td><td>24.02</td></tr></tbody></table></div><h2 id="分组实验"><a href="#分组实验" class="headerlink" title="分组实验"></a>分组实验</h2><div class="table-container"><table><thead><tr><th>分组实验</th><th>ACC</th><th>F1</th><th>Kappa</th><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>W \ N1,N2 \ N3,REM</td><td>0.858</td><td>0.834</td><td>0.809</td><td>0.921</td><td>0.598</td><td>0.887</td><td>0.916</td><td>0.848</td></tr><tr><td>W,N1 \ N2,N3,REM</td><td>0.858</td><td>0.819</td><td>0.809</td><td>0.905</td><td>0.508</td><td>0.889</td><td>0.924</td><td>0.868</td></tr><tr><td>W,N1,N2,N3 \ REM</td><td>0.854</td><td>0.821</td><td>0.803</td><td>0.908</td><td>0.577</td><td>0.893</td><td>0.921</td><td>0.804</td></tr><tr><td>W,N2,N3,REM \ N1</td><td>0.849</td><td>0.812</td><td>0.796</td><td>0.885</td><td>0.508</td><td>0.890</td><td>0.918</td><td>0.860</td></tr><tr><td>W \ N1,N2,N3 \ REM</td><td>0.852</td><td>0.822</td><td>0.801</td><td>0.892</td><td>0.544</td><td>0.881</td><td>0.916</td><td>0.876</td></tr><tr><td>W,REM \ N1,N2,N3</td><td>0.860</td><td>0.842</td><td>0.818</td><td>0.920</td><td>0.642</td><td>0.859</td><td>0.924</td><td>0.880</td></tr></tbody></table></div><h2 id="跨数据集"><a href="#跨数据集" class="headerlink" title="跨数据集"></a>跨数据集</h2><div class="table-container"><table><thead><tr><th>Train</th><th>Test</th><th>Model</th><th>ACC</th><th>F1</th><th>Kappa</th><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>ISRUC-S1</td><td>ISRUC-S3</td><td>StAGN</td><td>0.795</td><td>0.779</td><td></td><td>0.891</td><td>0.560</td><td>0.789</td><td>0.870</td><td>0.784</td></tr><tr><td>ISRUC-S1</td><td>ISRUC-S3</td><td>MVF-SleepNet</td><td>0.800</td><td>0.788</td><td></td><td>0.890</td><td>0.586</td><td>0.788</td><td>0.874</td><td>0.801</td></tr><tr><td>ISRUC-S1</td><td>ISRUC-S3</td><td>cVAN</td><td>0.826</td><td>0.807</td><td></td><td>0.916</td><td>0.578</td><td>0.814</td><td>0.889</td><td>0.843</td></tr><tr><td>ISRUC-S1</td><td>ISRUC-S3</td><td>S3Net</td><td>0.834</td><td>0.816</td><td>0.830</td><td>0.922</td><td>0.588</td><td>0.827</td><td>0.891</td><td>0.851</td></tr></tbody></table></div><h2 id="训练动态-收敛性与稳定性"><a href="#训练动态-收敛性与稳定性" class="headerlink" title="训练动态(收敛性与稳定性)"></a>训练动态(收敛性与稳定性)</h2><p>审稿人提到了没有讨论超参数以及训练动态的内容，但实际超参数的实验在正文中是有提及的，到时候引用一下就可以了，这里另外放上一份loss以及准确率的曲线</p><div style="display:flex;gap:12px;align-items:flex-start;margin:8px 0"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/loss_curve.png" style="width:50%;height:auto;object-fit:contain"> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/acc_curve.png" style="width:50%;height:auto;object-fit:contain"></div><p>从曲线中还是能看到，loss下降还算是平稳的，虽然后期过拟合了，但只需要在最低点保存权重即可，整体还是平稳的，很符合一个正常训练的loss的走势。此外，在PPG数据集的训练结果中也能看出，我们的模型对比cVAN有很大的提升，但cVAN的结果甚至没有超过ResNet，以及另一篇论文的结果的，所以能从更大的角度来说，我们的模型是普遍更优的，或者说对于不同性质的生理信号模型普遍更稳定效果更好</p><h2 id="统计学上的结果-类间方差以及混淆熵"><a href="#统计学上的结果-类间方差以及混淆熵" class="headerlink" title="统计学上的结果 (类间方差以及混淆熵)"></a>统计学上的结果 (类间方差以及混淆熵)</h2><h3 id="20-40岁受试者"><a href="#20-40岁受试者" class="headerlink" title="20-40岁受试者"></a>20-40岁受试者</h3><p>每类混淆熵的值如下</p><div class="table-container"><table><thead><tr><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>0.587743</td><td>1.504805</td><td>0.699646</td><td>0.517564</td><td>0.436499</td></tr></tbody></table></div><p>类间方差</p><div class="table-container"><table><thead><tr><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>0.114578</td><td>0.062937</td><td>0.009738</td><td>0.110816</td><td>0.030494</td></tr></tbody></table></div><h3 id="41-65岁受试者"><a href="#41-65岁受试者" class="headerlink" title="41-65岁受试者"></a>41-65岁受试者</h3><p>混淆熵</p><div class="table-container"><table><thead><tr><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>0.483451</td><td>1.466995</td><td>0.754874</td><td>0.598358</td><td>0.511859</td></tr></tbody></table></div><p>类间方差</p><div class="table-container"><table><thead><tr><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>0.14161</td><td>0.057958</td><td>0.012551</td><td>0.103621</td><td>0.021252</td></tr></tbody></table></div><h3 id="65岁以上受试者"><a href="#65岁以上受试者" class="headerlink" title="65岁以上受试者"></a>65岁以上受试者</h3><p>混淆熵</p><div class="table-container"><table><thead><tr><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>0.365262</td><td>1.287355</td><td>0.727059</td><td>0.805349</td><td>0.471778</td></tr></tbody></table></div><p>类间方差</p><div class="table-container"><table><thead><tr><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>0.18156</td><td>0.047594</td><td>0.012029</td><td>0.136241</td><td>0.023169</td></tr></tbody></table></div><h3 id="整体数据集"><a href="#整体数据集" class="headerlink" title="整体数据集"></a>整体数据集</h3><p>混淆熵</p><div class="table-container"><table><thead><tr><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>0.444948</td><td>1.404988</td><td>0.729318</td><td>0.624516</td><td>0.474131</td></tr></tbody></table></div><p>类间方差</p><div class="table-container"><table><thead><tr><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>0.141057</td><td>0.05344</td><td>0.010932</td><td>0.117301</td><td>0.034087</td></tr></tbody></table></div><p>混淆熵越大代表越混乱，类间方差越大，表示该类的“转移行为”与总体平均行为越不同，W 的类间方差最高，表明这个类别的转移行为与整体均值（五类平均转移）差异最大，行为模式比较“特殊”或“具有类区分度”。结合我们的转移矩阵以及睡眠的特征来说，睡眠是一个循循渐进的过程，从特质上说，一定是先从W到N1，再到N2，再到N3，再到REM这么一个逐渐变深的过程，所以我们选择将W N1 N2分成一组，N3 REM分成一组是有合理的依据的。并且从转移矩阵来说，W到N1，N1回W的双向转移概率都很大，而N3与N2之间只有N3到N2的概率略高。</p><div style="display:flex;gap:12px;align-items:flex-start;margin:8px 0"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/transition_probs_18_40.png" style="width:33%;height:auto;object-fit:contain"> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/transition_probs_41_65.png" style="width:33%;height:auto;object-fit:contain"> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/transition_probs_gt_65.png" style="width:33%;height:auto;object-fit:contain"></div><h2 id="缺模态"><a href="#缺模态" class="headerlink" title="缺模态"></a>缺模态</h2><p>先放一个暂时的结果，没跑完</p><div class="table-container"><table><thead><tr><th>Modality</th><th>SLEEPSMC (ICLR 2025 jia)</th><th>S3Net</th></tr></thead><tbody><tr><td>EEG</td><td>0.7646</td><td>0.8454</td></tr><tr><td>EOG</td><td>0.7444</td><td>0.8198</td></tr><tr><td>EMG</td><td>0.4384</td><td>0.5681</td></tr></tbody></table></div><h1 id="给评委的回复"><a href="#给评委的回复" class="headerlink" title="给评委的回复"></a>给评委的回复</h1><h2 id="评委1-Summary"><a href="#评委1-Summary" class="headerlink" title="评委1 Summary:"></a>评委1 Summary:</h2><p>This paper proposes a deep learning model called S3Net (Stage-Aware Sleep Staging Network) for automatic sleep staging. The authors point out two main problems with existing methods: (1) the model struggles to distinguish the subtle differences between light sleep stages (N1, N2); and (2) the fusion of temporal and frequency domain features is insufficient.</p><p>Soundness: 3: good<br>Presentation: 3: good<br>Contribution: 2: fair</p><p>Strengths:</p><ul><li>The proposed SAE module is the first to explicitly introduce an “expert-based stage difficulty” mechanism in sleep staging tasks, combined with Cross-Gate dynamic weight allocation, demonstrating a clear and highly inspiring approach.</li><li>The t-ALN module achieves temporal alignment of frequency and temporal features through learnable projection, effectively solving the problem of temporal structure loss after Transformer feature flattening.</li><li>Comparison with 15 state-of-the-art (SOTA) models shows that the results significantly outperform existing methods on three public datasets. Ablation experiments and visualizations (t-SNE, confusion matrix, spectrogram) demonstrate the design’s rationality and stability.</li></ul><h3 id="The-contributions-in-the-introduction-are-too-concise-further-refinement-is-recommended"><a href="#The-contributions-in-the-introduction-are-too-concise-further-refinement-is-recommended" class="headerlink" title="The contributions in the introduction are too concise; further refinement is recommended."></a>The contributions in the introduction are too concise; further refinement is recommended.</h3><p>这里详情可见我们修改后的论文</p><h3 id="While-the-t-ALN-and-SAE-module-designs-are-intuitive-the-paper-lacks-a-theoretical-explanation-or-complexity-analysis-of-their-underlying-principles"><a href="#While-the-t-ALN-and-SAE-module-designs-are-intuitive-the-paper-lacks-a-theoretical-explanation-or-complexity-analysis-of-their-underlying-principles" class="headerlink" title="While the t-ALN and SAE module designs are intuitive, the paper lacks a theoretical explanation or complexity analysis of their underlying principles."></a>While the t-ALN and SAE module designs are intuitive, the paper lacks a theoretical explanation or complexity analysis of their underlying principles.</h3><p>理论解释可见我们的方法部分，已经修改了对理论的解释。而关于模型复杂度的内容，下面的表格是我们的模型的参数与我们的baseline之间的对比，这里需要说明的是，MVF-SleepNet不是一个端到端的模型，所以我们就并没有对该模型的训练以及推理时间进行测试，只测试了它的参数量以及复杂度，而训练以及推理时间，我们是在NVIDIA RTX A6000上进行的测试，batch size = 32的情况下测试得到的结果，我们分别进行了1000次的运算并统计时间，得到的平均训练以及推理时间。下面是我们的表格。</p><div class="table-container"><table><thead><tr><th>model</th><th>Params(Million)</th><th>GFlops</th><th>train(ms)</th><th>infer(ms)</th></tr></thead><tbody><tr><td>MVF</td><td>39.51</td><td>11.06</td><td></td><td></td></tr><tr><td>cVAN</td><td>7.58</td><td>0.47</td><td>157.01</td><td>66.72</td></tr><tr><td>S3Net</td><td>6.49</td><td>2.70</td><td>75.69</td><td>24.02</td></tr></tbody></table></div><p>从表格中可以看到，我们的模型的参数量比以往的baseline中更小，在GFlops与上一个SOTA对比，虽然我们的复杂度略大于它，但仍保持在同一数量级。并且我们在世纪运行的训练速度与推理速度都要快于它，这也验证了我们的模型的效率。所以我们的模型相比他们的模型是更高效的</p><h3 id="Although-it-covers-three-commonly-used-datasets-it-remains-to-be-seen-whether-it-can-be-applied-to-other-physiological-signals-such-as-PPG"><a href="#Although-it-covers-three-commonly-used-datasets-it-remains-to-be-seen-whether-it-can-be-applied-to-other-physiological-signals-such-as-PPG" class="headerlink" title="Although it covers three commonly used datasets, it remains to be seen whether it can be applied to other physiological signals, such as PPG."></a>Although it covers three commonly used datasets, it remains to be seen whether it can be applied to other physiological signals, such as PPG.</h3><p>关于提到的PPG数据集，这里我们选择了一个单导PPG的四分类数据集，叫做<code>WESAD</code>,我们在这个数据集上进行了测试，并且选取了我们论文中提到的baseline以及近些年的一些工作，需要单独说明的是，我们的模型中有一部分是专门为睡眠分期设计的，即我们的 Stage-Aware Experts (SAE) 模块，故在此处我们选择部分删除了这个模块，只保留了SAE中的门控模块，然后将最后负责输出权重的部分换成了分类头。我们模型的测试是在此技术上进行的测试，下面是我们的结果</p><div class="table-container"><table><thead><tr><th>model</th><th>ACC</th><th>AUROC</th><th>AUPRC</th><th>F1</th><th>Kappa</th></tr></thead><tbody><tr><td>REBAR (ICLR 2024)</td><td>0.418</td><td>0.698</td><td>0.446</td><td></td><td></td></tr><tr><td>cVAN</td><td>0.6914</td><td>0.8550</td><td>0.7104</td><td>0.6641</td><td>0.5716</td></tr><tr><td>Resnet</td><td>0.713</td><td>0.877</td><td>0.746</td><td>0.682</td><td>0.593</td></tr><tr><td>HuBERT (NeurIPS 2025)</td><td>0.775</td><td>0.82</td><td></td><td></td><td></td></tr><tr><td>S3Net</td><td>0.853</td><td>0.911</td><td>0.833</td><td>0.819</td><td>0.787</td></tr></tbody></table></div><p>可以看到，尽管我们的模型虽然删去了我们精心设计的专家模块，但仍保持了高准确率，相比我们选择的睡眠模型的baseline以及原本为这个任务设计的模型，我们的模型都取得了更高的性能收益，更加的精准，所以能够看出我们的模型相比我们的baseline是普遍更优，且在多种任务的表现中更加稳定，也相比其他原本为PPG设计的模型得到的效果要更好，证明我们的架构在生理信号的时间序列领域的特征提取能力与建模能力是更优秀的。</p><h2 id="Reviewer-1-Response-polished"><a href="#Reviewer-1-Response-polished" class="headerlink" title="Reviewer 1 Response (polished)"></a>Reviewer 1 Response (polished)</h2><p>We thank the reviewer for the thoughtful evaluation and appreciate the positive assessment of our SAE and t-ALN designs, as well as the comprehensive comparisons and analyses. We respond to each point directly below.</p><ul><li><p>Clarify contributions in the introduction</p><ul><li>We have expanded the introduction to articulate our contributions more clearly: (i) a Stage-Aware Experts (SAE) mechanism with Cross-Gate dynamic weighting to address confusion between different sleep stages; (ii) a temporal Align-and-Link Network (t-ALN) that learnably projects and aligns temporal and spectral features while preserving local temporal structure; and (iii) extensive evaluation across three public datasets with ablations and visualizations supporting the design choices.</li></ul></li><li><p>Theory and complexity analysis for t-ALN and SAE</p><ul><li>The revised Methods section adds the formal description of t-ALN as a learnable projection that aligns sampling rates and sequence lengths across domains, and explains Cross-Gate as a task-conditioned importance estimator over experts. For complexity, we summarize params, GFLOPs, and measured runtime. MVF-SleepNet is not end-to-end, so we only report its params and GFLOPs. For cVAN and S3Net, we measured training and inference on an NVIDIA RTX A6000 (batch size 32) over 1000 runs and report averaged times. As shown above, S3Net uses fewer parameters than prior baselines and, while its GFLOPs are slightly higher than cVAN, they remain within the same order of magnitude; S3Net also trains and infers faster, indicating practical efficiency.</li></ul></li></ul><div class="table-container"><table><thead><tr><th>model</th><th>Params(Million)</th><th>GFlops</th><th>train(ms)</th><th>infer(ms)</th></tr></thead><tbody><tr><td>MVF</td><td>39.51</td><td>11.06</td><td></td><td></td></tr><tr><td>cVAN</td><td>7.58</td><td>0.47</td><td>157.01</td><td>66.72</td></tr><tr><td>S3Net</td><td>6.49</td><td>2.70</td><td>75.69</td><td>24.02</td></tr></tbody></table></div><ul><li>Applicability beyond sleep (e.g., PPG)<ul><li>To assess generalization, we evaluated on single-lead PPG (WESAD, 4-class). Because SAE’s stage-specific experts are sleep-oriented, we removed that part, retained the cross-gating mechanism, and replaced the output with a generic classification head. S3Net achieved 0.853 ACC, 0.911 AUROC, 0.833 AUPRC, 0.819 F1, and 0.787 Kappa, outperforming recent baselines, which supports the architecture’s broader utility for physiological time series.</li></ul></li></ul><div class="table-container"><table><thead><tr><th>model</th><th>ACC</th><th>AUROC</th><th>AUPRC</th><th>F1</th><th>Kappa</th></tr></thead><tbody><tr><td>REBAR (ICLR 2024)</td><td>0.418</td><td>0.698</td><td>0.446</td><td></td><td></td></tr><tr><td>cVAN</td><td>0.6914</td><td>0.8550</td><td>0.7104</td><td>0.6641</td><td>0.5716</td></tr><tr><td>Resnet</td><td>0.713</td><td>0.877</td><td>0.746</td><td>0.682</td><td>0.593</td></tr><tr><td>HuBERT (NeurIPS 2025)</td><td>0.775</td><td>0.82</td><td></td><td></td><td></td></tr><tr><td>S3Net</td><td>0.853</td><td>0.911</td><td>0.833</td><td>0.819</td><td>0.787</td></tr></tbody></table></div><h2 id="评委2-Summary"><a href="#评委2-Summary" class="headerlink" title="评委2 Summary:"></a>评委2 Summary:</h2><p>This paper presents S³Net (Stage-Aware Sleep Staging Network), which aims to address two major challenges in automatic sleep staging: (1) the difficulty in distinguishing transitional stages (particularly N1 and N2) due to overlapping EEG characteristics, and (2) the structural misalignment between temporal and spectral feature representations in existing time–frequency fusion networks. To tackle these issues, the authors propose two main modules: a time-alignment network (t-ALN) that projects spectral features onto the temporal axis for synchronized time–frequency fusion, and a stage-aware expert (SAE) mechanism that separates easily and hardly distinguishable stages into two expert branches. Experiments on multiple public datasets (ISRUC-S1/S3, Sleep-EDF) show improved accuracy and F1 scores compared to recent baselines such as cVAN and MixSleepNet.</p><p>Soundness: 2: fair<br>Presentation: 2: fair<br>Contribution: 2: fair</p><p>Strengths:</p><ul><li>The paper identifies two relevant challenges in EEG-based sleep staging and provides an intuitive design to address them. The structure of the paper and ablation study organization are clear and easy to follow.</li><li>Experiments cover multiple datasets and baselines, and the ablation study demonstrates that both proposed modules contribute positively to overall performance.</li><li>S³Net achieves consistent, albeit moderate, gains over strong baselines like cVAN and MixSleepNet across different datasets, especially in the N1/N2 stages that are known to be challenging.</li></ul><p>Weaknesses:</p><h3 id="Limited-methodological-novelty-The-two-problems-discussed—stage-heterogeneity-and-time–frequency-misalignment—are-well-known-and-have-already-been-addressed-by-previous-works-e-g-cVAN-MVF-SleepNet-The-proposed-t-ALN-essentially-performs-a-learnable-projection-and-sequence-reshaping-rather-than-a-true-signal-level-alignment-and-the-SAE-module-resembles-a-task-driven-Mixture-of-Experts-formulation-While-effective-these-ideas-are-incremental-rather-than-fundamentally-new"><a href="#Limited-methodological-novelty-The-two-problems-discussed—stage-heterogeneity-and-time–frequency-misalignment—are-well-known-and-have-already-been-addressed-by-previous-works-e-g-cVAN-MVF-SleepNet-The-proposed-t-ALN-essentially-performs-a-learnable-projection-and-sequence-reshaping-rather-than-a-true-signal-level-alignment-and-the-SAE-module-resembles-a-task-driven-Mixture-of-Experts-formulation-While-effective-these-ideas-are-incremental-rather-than-fundamentally-new" class="headerlink" title="Limited methodological novelty. The two problems discussed—stage heterogeneity and time–frequency misalignment—are well-known and have already been addressed by previous works (e.g., cVAN, MVF-SleepNet). The proposed t-ALN essentially performs a learnable projection and sequence reshaping rather than a true signal-level alignment, and the SAE module resembles a task-driven Mixture-of-Experts formulation. While effective, these ideas are incremental rather than fundamentally new."></a>Limited methodological novelty. The two problems discussed—stage heterogeneity and time–frequency misalignment—are well-known and have already been addressed by previous works (e.g., cVAN, MVF-SleepNet). The proposed t-ALN essentially performs a learnable projection and sequence reshaping rather than a true signal-level alignment, and the SAE module resembles a task-driven Mixture-of-Experts formulation. While effective, these ideas are incremental rather than fundamentally new.</h3><h3 id="Unclear-mechanism-of-“frequency-to-time-projection-”-The-claim-that-t-ALN-“projects-spectral-features-onto-the-temporal-axis”-is-conceptually-strong-but-technically-ambiguous-The-implementation-relies-on-attention-weighting-and-a-Transformer-encoder-without-theoretical-grounding-that-guarantees-faithful-temporal-reconstruction-Thus-it-is-more-of-a-structural-alignment-than-an-actual-mapping-from-frequency-to-time-weakening-the-claimed-interpretability"><a href="#Unclear-mechanism-of-“frequency-to-time-projection-”-The-claim-that-t-ALN-“projects-spectral-features-onto-the-temporal-axis”-is-conceptually-strong-but-technically-ambiguous-The-implementation-relies-on-attention-weighting-and-a-Transformer-encoder-without-theoretical-grounding-that-guarantees-faithful-temporal-reconstruction-Thus-it-is-more-of-a-structural-alignment-than-an-actual-mapping-from-frequency-to-time-weakening-the-claimed-interpretability" class="headerlink" title="Unclear mechanism of “frequency-to-time projection.” The claim that t-ALN “projects spectral features onto the temporal axis” is conceptually strong but technically ambiguous. The implementation relies on attention weighting and a Transformer encoder, without theoretical grounding that guarantees faithful temporal reconstruction. Thus, it is more of a structural alignment than an actual mapping from frequency to time, weakening the claimed interpretability."></a>Unclear mechanism of “frequency-to-time projection.” The claim that t-ALN “projects spectral features onto the temporal axis” is conceptually strong but technically ambiguous. The implementation relies on attention weighting and a Transformer encoder, without theoretical grounding that guarantees faithful temporal reconstruction. Thus, it is more of a structural alignment than an actual mapping from frequency to time, weakening the claimed interpretability.</h3><h3 id="Inadequate-differentiation-from-cVAN-Although-cVAN-is-cited-as-a-baseline-the-paper-does-not-provide-a-controlled-comparison-isolating-the-proposed-modules-from-cVAN’s-cross-view-attention-mechanism-It-remains-unclear-whether-S³Net’s-improvement-stems-from-its-alignment-design-expert-division-or-simply-additional-parameters-The-contribution-beyond-cVAN-is-therefore-not-sufficiently-demonstrated"><a href="#Inadequate-differentiation-from-cVAN-Although-cVAN-is-cited-as-a-baseline-the-paper-does-not-provide-a-controlled-comparison-isolating-the-proposed-modules-from-cVAN’s-cross-view-attention-mechanism-It-remains-unclear-whether-S³Net’s-improvement-stems-from-its-alignment-design-expert-division-or-simply-additional-parameters-The-contribution-beyond-cVAN-is-therefore-not-sufficiently-demonstrated" class="headerlink" title="Inadequate differentiation from cVAN. Although cVAN is cited as a baseline, the paper does not provide a controlled comparison isolating the proposed modules from cVAN’s cross-view attention mechanism. It remains unclear whether S³Net’s improvement stems from its alignment design, expert division, or simply additional parameters. The contribution beyond cVAN is therefore not sufficiently demonstrated."></a>Inadequate differentiation from cVAN. Although cVAN is cited as a baseline, the paper does not provide a controlled comparison isolating the proposed modules from cVAN’s cross-view attention mechanism. It remains unclear whether S³Net’s improvement stems from its alignment design, expert division, or simply additional parameters. The contribution beyond cVAN is therefore not sufficiently demonstrated.</h3><h3 id="Overstated-claims-versus-empirical-gain-The-reported-improvements-over-state-of-the-art-methods-are-relatively-modest-≈1–2-accuracy-small-F1-gains-and-may-not-justify-the-complexity-of-the-proposed-architecture-Moreover-there-is-no-quantitative-analysis-proving-that-the-t-ALN-truly-enhances-“alignment-quality”-or-that-the-SAE-indeed-learns-distinct-stage-specific-knowledge"><a href="#Overstated-claims-versus-empirical-gain-The-reported-improvements-over-state-of-the-art-methods-are-relatively-modest-≈1–2-accuracy-small-F1-gains-and-may-not-justify-the-complexity-of-the-proposed-architecture-Moreover-there-is-no-quantitative-analysis-proving-that-the-t-ALN-truly-enhances-“alignment-quality”-or-that-the-SAE-indeed-learns-distinct-stage-specific-knowledge" class="headerlink" title="Overstated claims versus empirical gain. The reported improvements over state-of-the-art methods are relatively modest (≈1–2% accuracy, small F1 gains), and may not justify the complexity of the proposed architecture. Moreover, there is no quantitative analysis proving that the t-ALN truly enhances “alignment quality” or that the SAE indeed learns distinct stage-specific knowledge."></a>Overstated claims versus empirical gain. The reported improvements over state-of-the-art methods are relatively modest (≈1–2% accuracy, small F1 gains), and may not justify the complexity of the proposed architecture. Moreover, there is no quantitative analysis proving that the t-ALN truly enhances “alignment quality” or that the SAE indeed learns distinct stage-specific knowledge.</h3><p>睡眠分期模型的研究历史很悠久，虽然我们的提升的确不算特别巨大，但仍是一个好的结果，我们在所有数据集上对比我们的baseline的全部指标均是最优的结果。同时我们对以往的一些baseline以及我们自己的模型进行了参数量，计算复杂度以及训练推理时间的测试，结果如下</p><div class="table-container"><table><thead><tr><th>model</th><th>Params(Million)</th><th>GFlops</th><th>train(ms)</th><th>infer(ms)</th></tr></thead><tbody><tr><td>MVF</td><td>39.51</td><td>11.06</td><td></td><td></td></tr><tr><td>cVAN</td><td>7.58</td><td>0.47</td><td>157.01</td><td>66.72</td></tr><tr><td>S3Net</td><td>6.49</td><td>2.70</td><td>75.69</td><td>24.02</td></tr></tbody></table></div><p>我们的模型在参数量上小于以往的baseline，虽然计算复杂度略大于原先的模型，但仍处于同样的数量级，不过我们的模型在训练上的速度以及推理的速度要显著优于以往的baseline，所以能证明我们的模型结构用更小的训练以及测试开销，换得了更优的性能，并且模型体积更小，更适合端侧部署，推理延迟也更低。</p><p>此外，我们有两份可视化的结果以及我们的消融实验能证明，引入了t-ALN后是有收益的。（底部有相同内容不赘述）。而关于SAE学习分阶段的特征，正文中的可视化实验中有提及，我们用门控输出的权重特征做t-SNE可视化，以及统计门控分类的准确率结果，可以看到cross-gate的t-SNE可视化结果显示，两个专家类别间的界限明确，类内紧密，分类效果显著，此外门控分类准确率高达97.1%，证明门控分类准确，SAE中的两个专家确实学到了特定阶段的特征。</p><p>Response (polished)</p><p>We appreciate the reviewer’s concern regarding the magnitude of improvements and the need for quantitative evidence. Sleep staging is a mature area; while absolute gains are modest, our results consistently exceed baselines across all datasets and metrics. Efficiency-wise, S3Net has fewer parameters than prior baselines and comparable compute, while training and inference are significantly faster on NVIDIA RTX A6000 at batch size 32 (averaged over 1000 runs; MVF-SleepNet is not end-to-end, so we report only its params and GFLOPs). As summarized above: MVF 39.51M params, 11.06 GFLOPs; cVAN 7.58M params, 0.47 GFLOPs, 157.01 ms train, 66.72 ms infer; S3Net 6.49M params, 2.70 GFLOPs, 75.69 ms train, 24.02 ms infer.</p><p>For alignment quality and stage-specific learning: our heatmap and t-SNE analyses show that t-ALN preserves temporal context and yields clearer class separation than no-ALN; controlled ablations corroborate these benefits. For SAE, cross-gate t-SNE exhibits clear separation between the two experts with tight intra-class clusters, and the gate classification accuracy reaches 97.1%, indicating distinct stage-specific knowledge is learned.</p><p>Taken together, these findings support that the gains are consistent and practically meaningful, and that the added modules deliver a favorable accuracy–efficiency trade-off with interpretable stage-aware behavior.</p><h3 id="Lack-of-theoretical-or-physiological-justification-The-paper-assumes-that-transitional-sleep-stages-require-distinct-sub-models-but-does-not-provide-physiological-reasoning-or-statistical-evidence-e-g-inter-class-variance-confusion-entropy-supporting-this-assumption"><a href="#Lack-of-theoretical-or-physiological-justification-The-paper-assumes-that-transitional-sleep-stages-require-distinct-sub-models-but-does-not-provide-physiological-reasoning-or-statistical-evidence-e-g-inter-class-variance-confusion-entropy-supporting-this-assumption" class="headerlink" title="Lack of theoretical or physiological justification. The paper assumes that transitional sleep stages require distinct sub-models but does not provide physiological reasoning or statistical evidence (e.g., inter-class variance, confusion entropy) supporting this assumption."></a>Lack of theoretical or physiological justification. The paper assumes that transitional sleep stages require distinct sub-models but does not provide physiological reasoning or statistical evidence (e.g., inter-class variance, confusion entropy) supporting this assumption.</h3><p>Metzner et al. (2021) — In a quantitative EEG transition analysis, a dominant Wake→N1→N2 path was observed, with N1 frequently oscillating with N2; “the stage-to-stage probabilities describe a strong propensity for transitions from Wake to N1 and from there to N2.” By contrast, stages with deep slow-wave or rhythmic activity show greater persistence: N3 is characterized by highly synchronized slow waves, whereas REM exhibits sustained theta-band activity.</p><p>Claus Metzner, Achim Schilling, Maximilian Traxdorf, Holger Schulze, and Patrick Krauss. Sleep as a random walk: A super-statistical analysis of EEG data across sleep stages. Communications Biology, 4:1385, 2021. doi: 10.1038/s42003-021-02912-6. URL: <a target="_blank" rel="noopener" href="https://www.nature.com/articles/s42003-021-02912-6">https://www.nature.com/articles/s42003-021-02912-6</a>.</p><p>Wijesinghe &amp; Lima (2025) — This EEG study distinguishes “light” versus “deep” stages via temporal features. N1 acts as a transitional stage: “N1, serving as a transitional stage between wakefulness and deeper sleep, naturally exhibits unstable dynamics, which aligns with its low autocovariance values.” REM and N3 exhibit significantly longer temporal dependencies than lighter stages (N1 and wake), reflecting sustained oscillatory patterns (N3 slow waves, REM theta). N1 is also ambiguous in EEG and hard to classify; it “remains the least confidently predicted, often contributing to rejected epochs” due to its intermediate, mixed features.</p><p>Dhanushka Wijesinghe and Ivan T. Lima, Jr. A lightweight neural network based on memory and transition probability for accurate real-time sleep stage classification. Brain Sciences, 15(8):789, 2025. doi: 10.3390/brainsci15080789. URL: <a target="_blank" rel="noopener" href="https://www.mdpi.com/2076-3425/15/8/789">https://www.mdpi.com/2076-3425/15/8/789</a>.</p><p>EEG Normal Sleep (StatPearls, 2024) — “Stage N1: alpha rhythm is attenuated and replaced by low‑amplitude, mixed‑frequency activity; slow eye movements may be present; vertex sharp waves can appear.” “Stage N2: presence of sleep spindles or K‑complexes.”<br>These canonical definitions clarify that N1 carries wake‑like low‑amplitude mixed‑frequency activity while N2 is event‑defined, supporting that N1 lies on the transition between W and N2.</p><p>Reference: Adeel M. Khan, Ramola S. S., et al. EEG Normal Sleep. StatPearls, 2024. URL: <a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/books/NBK537023/">https://www.ncbi.nlm.nih.gov/books/NBK537023/</a></p><p>Physiological definitions and scoring rules — “When an arousal interrupts stage N2 sleep, score subsequent segments as stage N1 if the EEG exhibits low‑amplitude, mixed‑frequency activity … Continue to score stage N1 until there is evidence for another stage, usually stage N2 or stage R.” “Attenuation of alpha rhythm was determined to be the most valid electrophysiological marker of sleep onset.” “Signs of transition from wakefulness to sleep include alpha attenuation/slowing, vertex sharp waves, incipient sleep spindles, slow eye movements, and brief ‘microsleep’ periods of low‑amplitude 4–7 Hz mixed‑frequency EEG.”<br>These rules position N1 as the operational transition state bridging wakefulness and stable N2/R, consistent with its mixed features and short persistence.</p><p>Reference: American Academy of Sleep Medicine. The AASM Manual for the Scoring of Sleep and Associated Events (Version 2.1 update summary). 2017. URL: <a target="_blank" rel="noopener" href="https://aasm.org/wp-content/uploads/2017/11/Summary-of-Updates-in-v2.1-FINAL.pdf">https://aasm.org/wp-content/uploads/2017/11/Summary-of-Updates-in-v2.1-FINAL.pdf</a><br>Reference: Michael H. Silber, Sonia Ancoli‑Israel, Michael H. Bonnet, Sudhansu Chokroverty, Madeleine M. Grigg‑Damberger, Max Hirshkowitz, Sheldon Kapen, Sharon A. Keenan, Meir H. Kryger, Thomas Penzel, Mark R. Pressman, and Conrad Iber. The visual scoring of sleep in adults. Journal of Clinical Sleep Medicine, 3(2):121–131, 2007. doi: 10.5664/jcsm.26814. URL: <a target="_blank" rel="noopener" href="https://jcsm.aasm.org/doi/10.5664/jcsm.26814">https://jcsm.aasm.org/doi/10.5664/jcsm.26814</a><br>Reference: Jeffrey Zimmerman. Stability versus transitional changes in the EEG: From sleep to wakefulness. Journal of Clinical Sleep Medicine, 11(9):1067–1075, 2015. doi: 10.5664/jcsm.4616. URL: <a target="_blank" rel="noopener" href="https://jcsm.aasm.org/doi/10.5664/jcsm.4616">https://jcsm.aasm.org/doi/10.5664/jcsm.4616</a></p><p>而关于我们做出专家分类子模型的划分，生理学统计是有的，我们的正文中提到了一个转移矩阵，该矩阵的定义是从一个阶段转移到另一个阶段的概率值，从图中可以看到，N1阶段往W阶段以及N2阶段转移的概率很高，N3与REM阶段则更倾向于保持自稳定，自转移概率高。而提到的类间方差以及混淆熵，我们也对数据集进行了完整那个的统计，统计结果显示如下</p><p>18-40岁受试者</p><p>每类混淆熵的值如下</p><div class="table-container"><table><thead><tr><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>0.587743</td><td>1.504805</td><td>0.699646</td><td>0.517564</td><td>0.436499</td></tr></tbody></table></div><p>类间方差</p><div class="table-container"><table><thead><tr><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>0.114578</td><td>0.062937</td><td>0.009738</td><td>0.110816</td><td>0.030494</td></tr></tbody></table></div><p>41-65岁受试者</p><p>混淆熵</p><div class="table-container"><table><thead><tr><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>0.483451</td><td>1.466995</td><td>0.754874</td><td>0.598358</td><td>0.511859</td></tr></tbody></table></div><p>类间方差</p><div class="table-container"><table><thead><tr><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>0.14161</td><td>0.057958</td><td>0.012551</td><td>0.103621</td><td>0.021252</td></tr></tbody></table></div><p>65岁以上受试者</p><p>混淆熵</p><div class="table-container"><table><thead><tr><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>0.365262</td><td>1.287355</td><td>0.727059</td><td>0.805349</td><td>0.471778</td></tr></tbody></table></div><p>类间方差</p><div class="table-container"><table><thead><tr><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>0.18156</td><td>0.047594</td><td>0.012029</td><td>0.136241</td><td>0.023169</td></tr></tbody></table></div><p>整体数据集</p><p>混淆熵</p><div class="table-container"><table><thead><tr><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>0.444948</td><td>1.404988</td><td>0.729318</td><td>0.624516</td><td>0.474131</td></tr></tbody></table></div><p>类间方差</p><div class="table-container"><table><thead><tr><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>0.141057</td><td>0.05344</td><td>0.010932</td><td>0.117301</td><td>0.034087</td></tr></tbody></table></div><p>混淆熵越大代表越混乱，类间方差越大，表示该类的“转移行为”与总体平均行为越不同，W 的类间方差最高，表明这个类别的转移行为与整体均值（五类平均转移）差异最大，行为模式比较“特殊”或“具有类区分度”。结合我们的转移矩阵以及睡眠的特征来说，睡眠是一个循循渐进的过程，从特质上说，一定是先从W到N1，再到N2，再到N3，再到REM这么一个逐渐变深的过程，所以我们选择将W N1 N2分成一组，N3 REM分成一组是有合理的依据的。并且从转移矩阵来说，W到N1，N1回W的双向转移概率都很大，而N3与N2之间只有N3到N2的概率略高。而从AASM的定义来说，N1作为一个过渡态的阶段，需要有显著证据表明过渡到了另一个期，才会被更改标记，故而N1期包含了大量与W和N2相似的特征，所以我们选择这样划分</p><p>Questions:</p><h2 id="What-are-the-key-architectural-differences-between-S³Net’s-fusion-mechanism-and-cVAN’s-cross-view-attention-A-direct-replacement-or-ablation-t-ALN-→-cVAN-style-attention-under-the-same-backbone-would-help-validate-the-claimed-structural-advantage"><a href="#What-are-the-key-architectural-differences-between-S³Net’s-fusion-mechanism-and-cVAN’s-cross-view-attention-A-direct-replacement-or-ablation-t-ALN-→-cVAN-style-attention-under-the-same-backbone-would-help-validate-the-claimed-structural-advantage" class="headerlink" title="What are the key architectural differences between S³Net’s fusion mechanism and cVAN’s cross-view attention? A direct replacement or ablation (t-ALN → cVAN-style attention) under the same backbone would help validate the claimed structural advantage."></a>What are the key architectural differences between S³Net’s fusion mechanism and cVAN’s cross-view attention? A direct replacement or ablation (t-ALN → cVAN-style attention) under the same backbone would help validate the claimed structural advantage.</h2><p>我们的对齐机制的核心思想是完整保存时频域（spectral）特征，然后将这条分支的特征完整且同时间关系地与时域特征进行交互，这不是简单的投影或重排。通过我们精心设计平方-加权-重排列-位置编码-注意力计算的流程，将时频域特征图彻底重构成完整且原生的的查询token矢量序列，这一系列token后续会经过与时间域特征相同的处理，最终进入了核心的窗口注意力。而cVAN式的对齐在构建的过程中先将时频域特征压缩成标量，然后在后续的计算过程中再次通过广播成一个token矢量，这一过程损失了部分源自时频域的特征信息，并且它并没有在构建的过程中有选择性的保留特征，直接选择max pooling保留最大值。而我们的做法是先将原始的频谱图进行能量化，赋予其以物理意义，再进行双维度”通道-频率”双加权，提亮更重要的特征，通过这些操作，为我们的token完整的保留了时频域的信息，在此基础上我们还保证了时间上的关系与时域是一致的，据此构建了更原生适配双维度的对齐模块。<br>此外，我们进行了一系列在我们的对齐模块上的消融实验，其中包括使用cVAN式的对齐方式，直接替换到我们的结构中，需要注意的是，此前提到的cVAN是将特征压缩成标量，然后在后续计算时进行广播升维，所以在我们替换实验中也保留了这一过程，以保证形状与我们模型后续的处理一致。下面是我们的实验结果。</p><div class="table-container"><table><thead><tr><th>version</th><th>ACC</th><th>F1</th><th>Kappa</th></tr></thead><tbody><tr><td>cVAN-like-ALN</td><td>0.858</td><td>0.820</td><td>0.812</td></tr><tr><td>pooling-ALN</td><td>0.861</td><td>0.831</td><td>0.816</td></tr><tr><td>flat-ALN</td><td>0.847</td><td>0.813</td><td>0.790</td></tr><tr><td>S3Net</td><td>0.866</td><td>0.855</td><td>0.827</td></tr></tbody></table></div><p>从实验中可以看到，替换成cVAN式的对齐后，模型的性能下降了，下降的原因能从此前提及的对齐结构差异中找到答案。并且我们另外进行了两个实验，一个是将我们对齐中的softmax加权方式替换成pooling，其余保持不变，这同样导致了性能的下降。我们的结构中的权重本质是沿频率与通道的注意力，而softmax 是标准的注意力归一化，可解释性与经验表现更佳。而直接展平再加入强力的位置编码，这一操作导致了更加严重的性能下滑，因为展平的过程严重破坏了时间前后关系，导致频域特征接近失效，所以得到了较差的结果，而添加进去的位置编码并不能挽回这一点，所以性能下滑严重。</p><h3 id="Please-provide-quantitative-or-visual-evidence-that-demonstrates-improved-time–frequency-alignment-after-introducing-t-ALN-e-g-correlation-heatmaps-or-reconstruction-error"><a href="#Please-provide-quantitative-or-visual-evidence-that-demonstrates-improved-time–frequency-alignment-after-introducing-t-ALN-e-g-correlation-heatmaps-or-reconstruction-error" class="headerlink" title="Please provide quantitative or visual evidence that demonstrates improved time–frequency alignment after introducing t-ALN (e.g., correlation heatmaps or reconstruction error)."></a>Please provide quantitative or visual evidence that demonstrates improved time–frequency alignment after introducing t-ALN (e.g., correlation heatmaps or reconstruction error).</h3><p>关于这一点可见我们正文中的可视化结果，这里进行了两组可视化实验，一组是t-ALN与no-ALN的可视化热力图对比，可见在我们模块的可视化中，能清晰的看到时间前后文的关系被完整的保留了，并且不同的类之间有显著的差异，而在no-ALN的热力图可视化中，所呈现的特征是杂乱无章的，这样的特征与时域交互时无疑接近噪声，很难学到有效的特征。本来有序的特征被打乱，被打乱的特征大致情况可见introduction。另一组实验是我们对三个特征进行了三种情况下的可视化结果。(a) No Cross-Layer Interaction(no fusion between X1Q,X2Q and X1T,X2T), (b)No-ALN，(c)Full S3Net。引入t-ALN后的对比可视化结果可见三份图中的(b)No-ALN与(c)Full S3Net，引入后t-SNE图不同类间的间隔是更开的，而类内更紧密。而另外可以分别看到X3E与X3Q中的(c),在引入了t-ALN之后，类见的分离度更明显，证明我们的t-ALN对特征是有显著改善的。</p><h3 id="Given-that-the-gains-are-relatively-modest-could-the-authors-discuss-whether-the-added-model-complexity-dual-experts-cross-gate-brings-a-favorable-accuracy–efficiency-trade-off"><a href="#Given-that-the-gains-are-relatively-modest-could-the-authors-discuss-whether-the-added-model-complexity-dual-experts-cross-gate-brings-a-favorable-accuracy–efficiency-trade-off" class="headerlink" title="Given that the gains are relatively modest, could the authors discuss whether the added model complexity (dual experts, cross-gate) brings a favorable accuracy–efficiency trade-off?"></a>Given that the gains are relatively modest, could the authors discuss whether the added model complexity (dual experts, cross-gate) brings a favorable accuracy–efficiency trade-off?</h3><p>睡眠分期检测这个领域经过多年的发展已经是一个很成熟的领域，我们在这里能得到相比之前的模型提升幅度的确不算巨大，但在成熟赛道里取得稳定的、跨数据集的1–3点增益，属于高基线上的有效提升，这转化为在高精度状态下显著降低了错误率。重要的是，添加的模块（双专家、交叉门）使模型保持在轻量级的预算范围内：参数量比此前的架构更小，GFLOP与广泛使用的架构相当，在相同的硬件和批处理设置下，测量的推理延迟仍然具有高度竞争力。下面是我们模型对比一些基线的模型参数，计算复杂度，训练时间，推理时间的实验对比，这里需要说明的是，MVF-SleepNet不是一个端到端的模型，所以我们并没有对该模型的训练以及推理时间进行测试，只测试了它的参数量以及复杂度，而训练以及推理时间，我们是在NVIDIA RTX A6000上进行的测试，batch size = 32的情况下测试得到的结果，我们分别进行了1000次的运算并统计时间，得到的平均训练以及推理时间。下面是我们的表格。</p><div class="table-container"><table><thead><tr><th>model</th><th>Params(Million)</th><th>GFlops</th><th>train(ms)</th><th>infer(ms)</th></tr></thead><tbody><tr><td>MVF</td><td>39.51</td><td>11.06</td><td></td><td></td></tr><tr><td>cVAN</td><td>7.58</td><td>0.47</td><td>157.01</td><td>66.72</td></tr><tr><td>S3Net</td><td>6.49</td><td>2.70</td><td>75.69</td><td>24.02</td></tr></tbody></table></div><p>具体来说，我们相比此前的模型，在更小的参数量，虽然模型复杂度略高，但我们的平均训练时间与推理时间分别是此前模型的1/2和1/3，这说明我们的模型用更小的训练开销，以及更小的参数量，更快的推理速度，得到了更佳的性能。</p><h2 id="Reviewer-2-Response-polished"><a href="#Reviewer-2-Response-polished" class="headerlink" title="Reviewer 2 Response (polished)"></a>Reviewer 2 Response (polished)</h2><p>We thank the reviewer for the careful assessment and appreciate the recognition of our paper’s organization, ablations, and consistent gains—especially on the challenging N1/N2 stages. We respond to the weaknesses and questions separately, following the reviewer’s order.</p><ul><li>Q1. Architectural differences and direct replacement/ablation<ul><li>Core mechanism: We reconstruct the time–frequency feature map into a native sequence of query tokens; these tokens then follow the same processing path as the temporal branch and finally enter the core windowed attention. This is not a simple projection or reshaping. Two architectural details are as follows.</li><li>t-ALN pipeline: square–energy normalization → dual weighting along channel and frequency → re-ordering → positional encoding → attention. This preserves full spectral detail and temporal order for faithful time–frequency interaction.</li><li>cVAN-style alignment: max-pooling compresses spectral features to scalars and later broadcasts them to token vectors, which loses fine spectral information and does not selectively retain features or guarantee temporal coherence.</li></ul></li></ul><div class="table-container"><table><thead><tr><th>version</th><th>ACC</th><th>F1</th><th>Kappa</th></tr></thead><tbody><tr><td>cVAN-like-ALN</td><td>0.858</td><td>0.820</td><td>0.812</td></tr><tr><td>pooling-ALN</td><td>0.861</td><td>0.831</td><td>0.816</td></tr><tr><td>flat-ALN</td><td>0.847</td><td>0.813</td><td>0.790</td></tr><tr><td>S3Net</td><td>0.866</td><td>0.855</td><td>0.827</td></tr></tbody></table></div><ul><li>Controlled experiments (same backbone, matched token shapes, identical training schedule and dataset splits):<ul><li>cVAN-like-ALN: replace our t-ALN with cVAN-style compress-to-scalar (max-pooling) followed by broadcast back to token vectors; we keep compress/broadcast to ensure downstream shapes match. ACC/F1/Kappa = 0.858/0.820/0.812 (Δ vs Full S3Net: −0.008/−0.035/−0.015).</li><li>Additional experiment 1 : keep the spectral-token reconstruction pipeline unchanged, but replace softmax-based dual weighting with learnable pooling; ACC/F1/Kappa = 0.861/0.831/0.816 (Δ: −0.005/−0.024/−0.011). Softmax acts as attention normalization along frequency/channel and provides better interpretability and empirical performance than pooling.</li><li>Additional experiment 2 : directly flatten the spectral map and add strong positional encoding, removing the dual weighting; ACC/F1/Kappa = 0.847/0.813/0.790 (Δ: −0.019/−0.042/−0.037). Flattening breaks temporal locality, making frequency-domain features nearly ineffective; positional encoding cannot recover the lost structure.</li><li>Full S3Net: energy normalization → dual channel–frequency weighting → re-ordering → positional encoding → windowed attention; ACC/F1/Kappa = 0.866/0.855/0.827.</li></ul></li><li>Under matched settings, all replacements lead to consistent drops, confirming that the gains come from detail-preserving spectral-token reconstruction and stage-aware gating—rather than parameter count.</li></ul><ul><li><p>Q2. Evidence of improved time–frequency alignment</p><ul><li>We provide two visual analyses in the main paper. First, t-ALN vs no-ALN heatmaps: with t-ALN, temporal context is clearly preserved and class-wise patterns are distinct; with no-ALN, features appear disordered and behave like noise when interacting with the temporal branch, making effective learning difficult. The disruption of originally ordered features is illustrated in the introduction.</li><li>Second, t-SNE and feature views under three settings: (a) No Cross-Layer Interaction (no fusion between X1Q,X2Q and X1T,X2T), (b) No-ALN, (c) Full S3Net. After introducing t-ALN, inter-class margins widen and intra-class cohesion tightens; in X3E and X3Q under (c), class separability becomes clearer, evidencing the improvement brought by t-ALN.</li></ul></li><li><p>Q3. Accuracy–efficiency trade-off</p><ul><li>Sleep staging is a mature field; achieving stable, cross-dataset gains of 1–3 points constitutes an effective improvement on high baselines and translates into a notable error reduction at high accuracy, especially for N1/N2.</li><li>Efficiency: the added modules (dual experts, cross-gate) keep the model within a lightweight budget—fewer parameters than prior architectures, GFLOPs in the same order, and competitive measured latency under identical hardware and batch settings. MVF-SleepNet is not end-to-end, so we report only its parameters and GFLOPs.</li><li>We measured training and inference on NVIDIA RTX A6000 with batch size 32 over 1000 runs and report averaged times (see table), yielding a favorable accuracy–efficiency balance: better accuracy with lower runtime and smaller model size.</li></ul></li></ul><div class="table-container"><table><thead><tr><th>model</th><th>Params(Million)</th><th>GFlops</th><th>train(ms)</th><th>infer(ms)</th></tr></thead><tbody><tr><td>MVF</td><td>39.51</td><td>11.06</td><td></td><td></td></tr><tr><td>cVAN</td><td>7.58</td><td>0.47</td><td>157.01</td><td>66.72</td></tr><tr><td>S3Net</td><td>6.49</td><td>2.70</td><td>75.69</td><td>24.02</td></tr></tbody></table></div><p>We appreciate the constructive questions and have aligned our response to the reviewer’s ordering.</p><h2 id="评委4-Summary"><a href="#评委4-Summary" class="headerlink" title="评委4 Summary:"></a>评委4 Summary:</h2><p>The authors introduce S3Net, an approach for automated sleep staging, crucial in diagnosing sleep disorders and analyzing sleep patterns. S3Net addresses challenges in distinguishing transitional sleep stages and integrating time-domain and frequency-domain information effectively. It comprises a Stage-Aware Experts module that divides sleep stages into distinct groups for specialized processing and a Time Alignment Module for aligning frequency-derived features with the temporal axis. Evaluation on three datasets demonstrates S3Net’s superior performance, achieving high accuracy and marked improvements in classifying challenging stages like N1 and N2. Ablation studies confirm the effectiveness of each module, with the full S3Net model showcasing the best results.</p><p>Soundness: 3: good<br>Presentation: 3: good<br>Contribution: 2: fair<br>Strengths:</p><p>The framework is clearly presented with a clean modular design. The workflow (signal encoding → t-ALN → SAE → classification) is easy to follow, and the algorithmic components are described in sufficient detail.</p><p>While both attention-based fusion and expert gating are known techniques, their integration within the context of sleep staging is coherent and well-implemented. The proposed modules complement each other, improving stage-level discriminability and robustness.</p><p>The model achieves stable gains across multiple datasets and metrics. The ablation and t-SNE analyses show internally consistent patterns, reflecting careful engineering and experimental execution, though some evaluation aspects still require refinement.</p><p>Weaknesses:</p><h3 id="The-ablation-only-studies-the-presence-absence-of-t-ALN-and-SAE-It-would-be-helpful-to-explore-variations-such-as-the-number-of-experts-or-alternative-attention-weighting-schemes"><a href="#The-ablation-only-studies-the-presence-absence-of-t-ALN-and-SAE-It-would-be-helpful-to-explore-variations-such-as-the-number-of-experts-or-alternative-attention-weighting-schemes" class="headerlink" title="The ablation only studies the presence/absence of t-ALN and SAE. It would be helpful to explore variations such as the number of experts or alternative attention weighting schemes."></a>The ablation only studies the presence/absence of t-ALN and SAE. It would be helpful to explore variations such as the number of experts or alternative attention weighting schemes.</h3><h3 id="All-experiments-are-within-domain-on-similar-PSG-datasets-EEG-dominant-Cross-dataset-or-missing-modality-tests-would-better-demonstrate-generalization"><a href="#All-experiments-are-within-domain-on-similar-PSG-datasets-EEG-dominant-Cross-dataset-or-missing-modality-tests-would-better-demonstrate-generalization" class="headerlink" title="All experiments are within-domain on similar PSG datasets (EEG-dominant). Cross-dataset or missing-modality tests would better demonstrate generalization."></a>All experiments are within-domain on similar PSG datasets (EEG-dominant). Cross-dataset or missing-modality tests would better demonstrate generalization.</h3><p>我们实际上已经有针对专家数量的消融实验，详见我们的超参数实验的部分。另外关于替代注意力加权的方案或者其他的消融实验，我们准备了以下的一些实验</p><div class="table-container"><table><thead><tr><th>version</th><th>ACC</th><th>F1</th><th>Kappa</th></tr></thead><tbody><tr><td>cVAN-like-ALN</td><td>0.858</td><td>0.820</td><td>0.812</td></tr><tr><td>pooling-ALN</td><td>0.861</td><td>0.831</td><td>0.816</td></tr><tr><td>flat-ALN</td><td>0.847</td><td>0.813</td><td>0.790</td></tr><tr><td>S3Net</td><td>0.866</td><td>0.855</td><td>0.827</td></tr></tbody></table></div><ul><li>Controlled experiments (same backbone, matched token shapes, identical training schedule and dataset splits):<ul><li>cVAN-like-ALN: replace our t-ALN with cVAN-style compress-to-scalar (max-pooling) followed by broadcast back to token vectors; we keep compress/broadcast to ensure downstream shapes match. ACC/F1/Kappa = 0.858/0.820/0.812 (Δ vs Full S3Net: −0.008/−0.035/−0.015).</li><li>Additional experiment 1 : keep the spectral-token reconstruction pipeline unchanged, but replace softmax-based dual weighting with learnable pooling; ACC/F1/Kappa = 0.861/0.831/0.816 (Δ: −0.005/−0.024/−0.011). Softmax acts as attention normalization along frequency/channel and provides better interpretability and empirical performance than pooling.</li><li>Additional experiment 2 : directly flatten the spectral map and add strong positional encoding, removing the dual weighting; ACC/F1/Kappa = 0.847/0.813/0.790 (Δ: −0.019/−0.042/−0.037). Flattening breaks temporal locality, making frequency-domain features nearly ineffective; positional encoding cannot recover the lost structure.</li><li>Full S3Net: energy normalization → dual channel–frequency weighting → re-ordering → positional encoding → windowed attention; ACC/F1/Kappa = 0.866/0.855/0.827.</li></ul></li><li>Under matched settings, all replacements lead to consistent drops, confirming that the gains come from detail-preserving spectral-token reconstruction and stage-aware gating—rather than parameter count.</li></ul></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E9%A1%B9%E7%9B%AE/">项目</a><a class="post-meta__tags" href="/tags/python/">python</a></div><div class="post-share"><div class="social-share" data-image="/img/head.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="prev-post pull-left" href="/2025/11/20/review/" title="review"><div class="cover" style="background:var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">review</div></div></a><a class="next-post pull-right" href="/2025/11/07/%E5%9B%A2%E9%98%9F%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/" title="团队开发入门指南"><div class="cover" style="background:var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">团队开发入门指南</div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a href="/2025/12/05/%E5%96%89%E9%95%9C%E6%8A%A5%E5%91%8A%E6%A8%A1%E5%9E%8B1.0/" title="喉镜报告模型1.0"><div class="cover" style="background:var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-05</div><div class="title">喉镜报告模型1.0</div></div></a><a href="/2025/11/20/review/" title="review"><div class="cover" style="background:var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-20</div><div class="title">review</div></div></a><a href="/2025/10/12/%E5%96%89%E9%95%9C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" title="喉镜数据分析"><div class="cover" style="background:var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-12</div><div class="title">喉镜数据分析</div></div></a><a href="/2025/11/02/%E5%96%89%E9%95%9C%E7%BB%93%E6%9E%84%E8%A7%82%E6%B5%8B%E6%B1%87%E6%8A%A5/" title="喉镜结构观测汇报"><div class="cover" style="background:var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-02</div><div class="title">喉镜结构观测汇报</div></div></a><a href="/2025/06/19/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E7%9D%A1%E7%9C%A0%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E9%93%BE/" title="多模态智能体的睡眠分析平台"><div class="cover" style="background:var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-19</div><div class="title">多模态智能体的睡眠分析平台</div></div></a><a href="/2025/06/28/%E9%A1%B9%E7%9B%AE%E5%91%A8%E8%AE%B0/" title="项目周记录"><div class="cover" style="background:var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-28</div><div class="title">项目周记录</div></div></a></div></div><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/head.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">xxxkkw</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">35</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxkkw"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">我的小站</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Rebuttal"><span class="toc-number">1.</span> <span class="toc-text">Rebuttal</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#PPG%E6%95%B0%E6%8D%AE%E5%88%86%E7%B1%BB%E7%BB%93%E6%9E%9C"><span class="toc-number">1.1.</span> <span class="toc-text">PPG数据分类结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E9%BD%90%E7%9A%84%E5%90%84%E7%A7%8D%E6%94%B9%E7%89%88"><span class="toc-number">1.2.</span> <span class="toc-text">对齐的各种改版</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E9%87%8F%E3%80%81%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6%E4%B8%8E%E6%97%B6%E9%97%B4"><span class="toc-number">1.3.</span> <span class="toc-text">参数量、计算复杂度与时间</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%BB%84%E5%AE%9E%E9%AA%8C"><span class="toc-number">1.4.</span> <span class="toc-text">分组实验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B7%A8%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.5.</span> <span class="toc-text">跨数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%8A%A8%E6%80%81-%E6%94%B6%E6%95%9B%E6%80%A7%E4%B8%8E%E7%A8%B3%E5%AE%9A%E6%80%A7"><span class="toc-number">1.6.</span> <span class="toc-text">训练动态(收敛性与稳定性)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%8A%E7%9A%84%E7%BB%93%E6%9E%9C-%E7%B1%BB%E9%97%B4%E6%96%B9%E5%B7%AE%E4%BB%A5%E5%8F%8A%E6%B7%B7%E6%B7%86%E7%86%B5"><span class="toc-number">1.7.</span> <span class="toc-text">统计学上的结果 (类间方差以及混淆熵)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#20-40%E5%B2%81%E5%8F%97%E8%AF%95%E8%80%85"><span class="toc-number">1.7.1.</span> <span class="toc-text">20-40岁受试者</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#41-65%E5%B2%81%E5%8F%97%E8%AF%95%E8%80%85"><span class="toc-number">1.7.2.</span> <span class="toc-text">41-65岁受试者</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#65%E5%B2%81%E4%BB%A5%E4%B8%8A%E5%8F%97%E8%AF%95%E8%80%85"><span class="toc-number">1.7.3.</span> <span class="toc-text">65岁以上受试者</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.7.4.</span> <span class="toc-text">整体数据集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E6%A8%A1%E6%80%81"><span class="toc-number">1.8.</span> <span class="toc-text">缺模态</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%99%E8%AF%84%E5%A7%94%E7%9A%84%E5%9B%9E%E5%A4%8D"><span class="toc-number">2.</span> <span class="toc-text">给评委的回复</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E5%A7%941-Summary"><span class="toc-number">2.1.</span> <span class="toc-text">评委1 Summary:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-contributions-in-the-introduction-are-too-concise-further-refinement-is-recommended"><span class="toc-number">2.1.1.</span> <span class="toc-text">The contributions in the introduction are too concise; further refinement is recommended.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#While-the-t-ALN-and-SAE-module-designs-are-intuitive-the-paper-lacks-a-theoretical-explanation-or-complexity-analysis-of-their-underlying-principles"><span class="toc-number">2.1.2.</span> <span class="toc-text">While the t-ALN and SAE module designs are intuitive, the paper lacks a theoretical explanation or complexity analysis of their underlying principles.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Although-it-covers-three-commonly-used-datasets-it-remains-to-be-seen-whether-it-can-be-applied-to-other-physiological-signals-such-as-PPG"><span class="toc-number">2.1.3.</span> <span class="toc-text">Although it covers three commonly used datasets, it remains to be seen whether it can be applied to other physiological signals, such as PPG.</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reviewer-1-Response-polished"><span class="toc-number">2.2.</span> <span class="toc-text">Reviewer 1 Response (polished)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E5%A7%942-Summary"><span class="toc-number">2.3.</span> <span class="toc-text">评委2 Summary:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Limited-methodological-novelty-The-two-problems-discussed%E2%80%94stage-heterogeneity-and-time%E2%80%93frequency-misalignment%E2%80%94are-well-known-and-have-already-been-addressed-by-previous-works-e-g-cVAN-MVF-SleepNet-The-proposed-t-ALN-essentially-performs-a-learnable-projection-and-sequence-reshaping-rather-than-a-true-signal-level-alignment-and-the-SAE-module-resembles-a-task-driven-Mixture-of-Experts-formulation-While-effective-these-ideas-are-incremental-rather-than-fundamentally-new"><span class="toc-number">2.3.1.</span> <span class="toc-text">Limited methodological novelty. The two problems discussed—stage heterogeneity and time–frequency misalignment—are well-known and have already been addressed by previous works (e.g., cVAN, MVF-SleepNet). The proposed t-ALN essentially performs a learnable projection and sequence reshaping rather than a true signal-level alignment, and the SAE module resembles a task-driven Mixture-of-Experts formulation. While effective, these ideas are incremental rather than fundamentally new.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Unclear-mechanism-of-%E2%80%9Cfrequency-to-time-projection-%E2%80%9D-The-claim-that-t-ALN-%E2%80%9Cprojects-spectral-features-onto-the-temporal-axis%E2%80%9D-is-conceptually-strong-but-technically-ambiguous-The-implementation-relies-on-attention-weighting-and-a-Transformer-encoder-without-theoretical-grounding-that-guarantees-faithful-temporal-reconstruction-Thus-it-is-more-of-a-structural-alignment-than-an-actual-mapping-from-frequency-to-time-weakening-the-claimed-interpretability"><span class="toc-number">2.3.2.</span> <span class="toc-text">Unclear mechanism of “frequency-to-time projection.” The claim that t-ALN “projects spectral features onto the temporal axis” is conceptually strong but technically ambiguous. The implementation relies on attention weighting and a Transformer encoder, without theoretical grounding that guarantees faithful temporal reconstruction. Thus, it is more of a structural alignment than an actual mapping from frequency to time, weakening the claimed interpretability.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Inadequate-differentiation-from-cVAN-Although-cVAN-is-cited-as-a-baseline-the-paper-does-not-provide-a-controlled-comparison-isolating-the-proposed-modules-from-cVAN%E2%80%99s-cross-view-attention-mechanism-It-remains-unclear-whether-S%C2%B3Net%E2%80%99s-improvement-stems-from-its-alignment-design-expert-division-or-simply-additional-parameters-The-contribution-beyond-cVAN-is-therefore-not-sufficiently-demonstrated"><span class="toc-number">2.3.3.</span> <span class="toc-text">Inadequate differentiation from cVAN. Although cVAN is cited as a baseline, the paper does not provide a controlled comparison isolating the proposed modules from cVAN’s cross-view attention mechanism. It remains unclear whether S³Net’s improvement stems from its alignment design, expert division, or simply additional parameters. The contribution beyond cVAN is therefore not sufficiently demonstrated.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Overstated-claims-versus-empirical-gain-The-reported-improvements-over-state-of-the-art-methods-are-relatively-modest-%E2%89%881%E2%80%932-accuracy-small-F1-gains-and-may-not-justify-the-complexity-of-the-proposed-architecture-Moreover-there-is-no-quantitative-analysis-proving-that-the-t-ALN-truly-enhances-%E2%80%9Calignment-quality%E2%80%9D-or-that-the-SAE-indeed-learns-distinct-stage-specific-knowledge"><span class="toc-number">2.3.4.</span> <span class="toc-text">Overstated claims versus empirical gain. The reported improvements over state-of-the-art methods are relatively modest (≈1–2% accuracy, small F1 gains), and may not justify the complexity of the proposed architecture. Moreover, there is no quantitative analysis proving that the t-ALN truly enhances “alignment quality” or that the SAE indeed learns distinct stage-specific knowledge.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Lack-of-theoretical-or-physiological-justification-The-paper-assumes-that-transitional-sleep-stages-require-distinct-sub-models-but-does-not-provide-physiological-reasoning-or-statistical-evidence-e-g-inter-class-variance-confusion-entropy-supporting-this-assumption"><span class="toc-number">2.3.5.</span> <span class="toc-text">Lack of theoretical or physiological justification. The paper assumes that transitional sleep stages require distinct sub-models but does not provide physiological reasoning or statistical evidence (e.g., inter-class variance, confusion entropy) supporting this assumption.</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#What-are-the-key-architectural-differences-between-S%C2%B3Net%E2%80%99s-fusion-mechanism-and-cVAN%E2%80%99s-cross-view-attention-A-direct-replacement-or-ablation-t-ALN-%E2%86%92-cVAN-style-attention-under-the-same-backbone-would-help-validate-the-claimed-structural-advantage"><span class="toc-number">2.4.</span> <span class="toc-text">What are the key architectural differences between S³Net’s fusion mechanism and cVAN’s cross-view attention? A direct replacement or ablation (t-ALN → cVAN-style attention) under the same backbone would help validate the claimed structural advantage.</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Please-provide-quantitative-or-visual-evidence-that-demonstrates-improved-time%E2%80%93frequency-alignment-after-introducing-t-ALN-e-g-correlation-heatmaps-or-reconstruction-error"><span class="toc-number">2.4.1.</span> <span class="toc-text">Please provide quantitative or visual evidence that demonstrates improved time–frequency alignment after introducing t-ALN (e.g., correlation heatmaps or reconstruction error).</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Given-that-the-gains-are-relatively-modest-could-the-authors-discuss-whether-the-added-model-complexity-dual-experts-cross-gate-brings-a-favorable-accuracy%E2%80%93efficiency-trade-off"><span class="toc-number">2.4.2.</span> <span class="toc-text">Given that the gains are relatively modest, could the authors discuss whether the added model complexity (dual experts, cross-gate) brings a favorable accuracy–efficiency trade-off?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reviewer-2-Response-polished"><span class="toc-number">2.5.</span> <span class="toc-text">Reviewer 2 Response (polished)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E5%A7%944-Summary"><span class="toc-number">2.6.</span> <span class="toc-text">评委4 Summary:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-ablation-only-studies-the-presence-absence-of-t-ALN-and-SAE-It-would-be-helpful-to-explore-variations-such-as-the-number-of-experts-or-alternative-attention-weighting-schemes"><span class="toc-number">2.6.1.</span> <span class="toc-text">The ablation only studies the presence&#x2F;absence of t-ALN and SAE. It would be helpful to explore variations such as the number of experts or alternative attention weighting schemes.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#All-experiments-are-within-domain-on-similar-PSG-datasets-EEG-dominant-Cross-dataset-or-missing-modality-tests-would-better-demonstrate-generalization"><span class="toc-number">2.6.2.</span> <span class="toc-text">All experiments are within-domain on similar PSG datasets (EEG-dominant). Cross-dataset or missing-modality tests would better demonstrate generalization.</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/12/05/%E5%96%89%E9%95%9C%E6%8A%A5%E5%91%8A%E6%A8%A1%E5%9E%8B1.0/" title="喉镜报告模型1.0">喉镜报告模型1.0</a><time datetime="2025-12-05T02:45:00.000Z" title="发表于 2025-12-05 10:45:00">2025-12-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/20/review/" title="review">review</a><time datetime="2025-11-20T06:00:00.000Z" title="发表于 2025-11-20 14:00:00">2025-11-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/17/%E9%A1%B9%E7%9B%AE%E6%B1%87%E6%8A%A58.0/" title="项目汇报8.0">项目汇报8.0</a><time datetime="2025-11-17T07:00:00.000Z" title="发表于 2025-11-17 15:00:00">2025-11-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/07/%E5%9B%A2%E9%98%9F%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/" title="团队开发入门指南">团队开发入门指南</a><time datetime="2025-11-07T07:30:00.000Z" title="发表于 2025-11-07 15:30:00">2025-11-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/04/%E6%A0%87%E7%AD%BE%E5%B7%A5%E5%85%B7%E6%95%99%E5%AD%A6%E5%90%91/" title="标签工具教学向">标签工具教学向</a><time datetime="2025-11-04T10:30:00.000Z" title="发表于 2025-11-04 18:30:00">2025-11-04</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By xxxkkw</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(()=>{const t=()=>{new Valine(Object.assign({el:"#vcomment",appId:"NuwmhVS9BmgOpiVJWroZWSWW-gzGzoHsz",appKey:"NJWb1pCdV9rcU5odlHizyxsJ",avatar:"monsterid",serverURLs:"",emojiMaps:"",path:window.location.pathname,visitor:!1},null))},e=async()=>{"function"==typeof Valine||await btf.getScript("https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"),t()};setTimeout(e,0)})()</script></div></div></body></html>