<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>review | xxxkkw的妙妙屋</title><meta name="author" content="xxxkkw"><meta name="copyright" content="xxxkkw"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="To reviewer ndSL: We thank the reviewer for the thoughtful evaluation and appreciate the positive assessment of our SAE and t-ALN designs, as well as the comprehensive comparisons and analyses. We res"><meta property="og:type" content="article"><meta property="og:title" content="review"><meta property="og:url" content="http://example.com/2025/11/20/review/index.html"><meta property="og:site_name" content="xxxkkw的妙妙屋"><meta property="og:description" content="To reviewer ndSL: We thank the reviewer for the thoughtful evaluation and appreciate the positive assessment of our SAE and t-ALN designs, as well as the comprehensive comparisons and analyses. We res"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/img/head.jpg"><meta property="article:published_time" content="2025-11-20T06:00:00.000Z"><meta property="article:modified_time" content="2025-11-20T05:48:06.229Z"><meta property="article:author" content="xxxkkw"><meta property="article:tag" content="项目"><meta property="article:tag" content="python"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://example.com/img/head.jpg"><link rel="shortcut icon" href="/img/wall.jpg"><link rel="canonical" href="http://example.com/2025/11/20/review/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{const e={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:e,getScript:(e,t={})=>new Promise((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach(([e,t])=>n.setAttribute(e,t)),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),getCSS:(e,t)=>new Promise((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),addGlobalFn:(e,t,o=!1,a=window)=>{if(e.startsWith("pjax"))return;const n=a.globalFn||{};n[e]=n[e]||{},o&&n[e][o]||(n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n)}};const t=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},o=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","ffffff")};btf.activateDarkMode=t,btf.activateLightMode=o;const a=e.get("theme");"dark"===a?t():"light"===a&&o();const n=e.get("aside-status");void 0!==n&&document.documentElement.classList.toggle("hide-aside","hide"===n);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:void 0,noticeOutdate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!0},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"review",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!1,postUpdate:"2025-11-20 13:48:06"}</script><link rel="stylesheet" href="/styles/main.css"><head><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.3.0"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{const d=document.getElementById("loading-box"),e=document.body,o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l(),window.addEventListener("load",o)})()</script><div id="web_bg" style="background-color:#efefef"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/head.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">34</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 文章</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(/img/wall.jpg)"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">xxxkkw的妙妙屋</span></a><a class="nav-page-title" href="/"><span class="site-name">review</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 文章</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">review</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-11-20T06:00:00.000Z" title="发表于 2025-11-20 14:00:00">2025-11-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-11-20T05:48:06.229Z" title="更新于 2025-11-20 13:48:06">2025-11-20</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2025/11/20/review/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2025/11/20/review/" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>To reviewer ndSL:</p><p>We thank the reviewer for the thoughtful evaluation and appreciate the positive assessment of our SAE and t-ALN designs, as well as the comprehensive comparisons and analyses. We respond to each point directly below.</p><ol><li>The concise of our introduction,the refinement are as follow</li></ol><p>Clarify contributions in the introduction<br>We have expanded the introduction to articulate our contributions more clearly: (i) a Stage-Aware Experts (SAE) mechanism with Cross-Gate dynamic weighting to address confusion between different sleep stages; (ii) a temporal Align-and-Link Network (t-ALN) that learnably projects and aligns temporal and spectral features while preserving local temporal structure; and (iii) extensive evaluation across three public datasets with ablations and visualizations supporting the design choices.</p><ol><li>The theory of t-ALN and SAE is unclarified?</li></ol><p>The revised Methods section adds the formal description of t-ALN as a learnable projection that aligns sampling rates and sequence lengths across domains, and explains Cross-Gate as a task-conditioned importance estimator over experts. For complexity, we summarize params, GFLOPs, and measured runtime. MVF-SleepNet is not end-to-end, so we only report its params and GFLOPs. For cVAN and S3Net, we measured training and inference on an NVIDIA RTX A6000 (batch size 32) over 1000 runs and report averaged times. As shown above, S3Net uses fewer parameters than prior baselines and, while its GFLOPs are slightly higher than cVAN, they remain within the same order of magnitude; S3Net also trains and infers faster, indicating practical efficiency.</p><div class="table-container"><table><thead><tr><th>model</th><th>Params(Million)</th><th>GFlops</th><th>train(ms)</th><th>infer(ms)</th></tr></thead><tbody><tr><td>MVF</td><td>39.51</td><td>11.06</td><td></td><td></td></tr><tr><td>cVAN</td><td>7.58</td><td>0.47</td><td>157.01</td><td>66.72</td></tr><tr><td>S3Net</td><td>6.49</td><td>2.70</td><td>75.69</td><td>24.02</td></tr></tbody></table></div><ol><li>Is the S3Net can be use in other physiological signals?</li></ol><p>Yes, to assess generalization, we evaluated on single-lead PPG (WESAD, 4-class). Because SAE’s stage-specific experts are sleep-oriented, we removed that part, retained the cross-gating mechanism, and replaced the output with a generic classification head. S3Net achieved 0.853 ACC, 0.911 AUROC, 0.833 AUPRC, 0.819 F1, and 0.787 Kappa, outperforming recent baselines, which supports the architecture’s broader utility for physiological time series.</p><div class="table-container"><table><thead><tr><th>model</th><th>ACC</th><th>AUROC</th><th>AUPRC</th><th>F1</th><th>Kappa</th></tr></thead><tbody><tr><td>REBAR (ICLR 2024)</td><td>0.418</td><td>0.698</td><td>0.446</td><td></td><td></td></tr><tr><td>cVAN</td><td>0.6914</td><td>0.8550</td><td>0.7104</td><td>0.6641</td><td>0.5716</td></tr><tr><td>Resnet</td><td>0.713</td><td>0.877</td><td>0.746</td><td>0.682</td><td>0.593</td></tr><tr><td>HuBERT (NeurIPS 2025)</td><td>0.775</td><td>0.82</td><td></td><td></td><td></td></tr><tr><td>S3Net</td><td>0.853</td><td>0.911</td><td>0.833</td><td>0.819</td><td>0.787</td></tr></tbody></table></div><p>To reviewer PzED:</p><p>Reply:</p><p>Thank you for recognizing and supporting the writing and experimental work in our paper.</p><ol><li>What are the essential novelty and architectural difference in our paper compared to the cVAN and the MVF-SleepNet?<br>Honestly, our model achieves a incredible novelty compared to cVAN and MVF-SleepNet.<br>We would like to clarify that our method introduces essential novelty beyond cVAN and MVF-SleepNet by explicitly modeling two long-standing challenges in sleep staging—stage heterogeneity and time–frequency misalignment—in ways that prior works have not addressed.<br>First, regarding stage heterogeneity: sleep stages follow a gradual and continuous progression (W → N1 → N2 → N3 → REM). Because N1 lies between wakefulness and deeper sleep stages, it is inherently ambiguous and frequently confused with both W and N2. This natural progression is well known, but previous models have not explicitly incorporated this structure into their architectures. Our SAE module is the first to transform this physiological insight into a principled modeling strategy. By grouping stages into (1) hard-to-separate stages with gradual transitions (W, N1, N2) and (2) easy-to-separate stages with clear, stable patterns (N3, REM), the SAE assigns specialized experts to each group in a task-driven manner. This is not a generic Mixture-of-Experts; it embeds domain knowledge about stage progression directly into the model architecture, enabling more accurate discrimination of inherently ambiguous stages.</li></ol><p>Second, concerning time–frequency misalignment: cVAN introduces a time-alignment mechanism, but it does not completely solve this issue. cVAN constructs time–frequency images and reorganizes feature positions so that they follow the temporal frame order, which helps the model capture features. However, the subsequent projection head and embedding layer may impair this temporal consistency. Specifically, cVAN first compresses the feature map of shape (H, W, D) into a 1D vector via a fully connected layer, and then applies an embedding layer to obtain a 2D representation compatible with the transformer. This operation can disrupt the temporal structure, weaken the ability to capture time–frequency relationships, and potentially hinder gradient flow due to the embedding layer.</p><p>MVF-SleepNet, on the other hand, simply sums features from different branches without deep integration, which does not fundamentally address time–frequency misalignment either and it also ignore the importance of stage heterogeneity.</p><p>In contrast, our novelty lies in the joint design of the T-ALN module and the SAE module:<br>The T-ALN module explicitly addresses time–frequency misalignment by reorganizing features into a 2D representation and enhancing their expressiveness. We then apply a time-window-wise linear projection to map each time-frame feature into the transformer layer. This preserves the original temporal order while transforming time–frequency features into an “energy” representation, enabling the model to capture the relationship between temporal and frequency information more effectively.<br>The SAE module explicitly targets stage heterogeneity. It is, to our knowledge, the first module that directly models this phenomenon at the architectural level. While many models acknowledge that N1 is difficult to classify (including cVAN), they typically treat this as an outcome rather than modeling its cause. Our SAE integrates a Mixture-of-Experts design with an explicit grouping of the five sleep stages into two expert branches: a “hard-to-separate” expert for W, N1, and N2 (which are frequently confused), and an “easy-to-separate” expert for N3 and REM (which have stable and distinct patterns).<br>Therefore, our novelty is both conceptual and architectural: we (1) propose the T-ALN module to more effectively handle time–frequency misalignment, and (2) propose the SAE module to explicitly model stage heterogeneity in a physiologically informed manner.</p><ol><li>The mechanism of “frequency-to-time projection” is unclear?<br>[We will further clarify this mechanism in the revised manuscript, including how features are projected from the frequency domain to a time-aligned representation and how this interacts with T-ALN and the transformer encoder]</li></ol><ol><li>What is the main difference compared to cVAN?<br>Thank you for your constructive comments. Here we emphasize the main differences between our method and cVAN, focusing on alignment design, specialized experts, and efficiency.<br>(1)The design of alignment:</li></ol><p>We implement alignment in a fundamentally different way. Our T-ALN module performs representation enhancement and applies a time-window-wise linear projection, which preserves the original temporal order while capturing time–frequency features more effectively. In this process, the frequency-domain features are transformed into an enhanced “energy” representation that strengthens the relationship between temporal and spectral information, rather than disrupting it.<br>While cVAN’s alignment inspired our ideas, it has several drawbacks. The final projection head in cVAN can destroy the temporal consistency that we aim to preserve and that is central to our contribution. In addition, its output is passed through an embedding layer, which may hinder efficient gradient propagation and weaken the time–frequency representation.<br>Our experiments further support the superiority of T-ALN. When we replace the T-ALN module with cVAN’s alignment, performance consistently drops, demonstrating that T-ALN is more effective. Additionally, Figure 5 and Figure 6 in our paper provide strong visual evidence (via heatmaps and t-SNE) that T-ALN learns meaningful time–frequency features. In particular, Figure 6 visualizes features using t-SNE under three alignment configurations: (a) no fusion between XQ1, XQ2 and XT1, XT2, (b) No-ALN, and (c) full S3Net. Comparing these configurations, we observe a significant improvement in feature separability when T-ALN is used.<br>The ablation results for different ALN variants are summarized below:</p><div class="table-container"><table><thead><tr><th>version</th><th>ACC</th><th>F1</th><th>Kappa</th></tr></thead><tbody><tr><td>cVAN-ALN</td><td>0.858</td><td>0.820</td><td>0.812</td></tr><tr><td>pooling-ALN</td><td>0.861</td><td>0.831</td><td>0.816</td></tr><tr><td>flat-ALN</td><td>0.847</td><td>0.813</td><td>0.790</td></tr><tr><td>no-weight &amp; power -ALN</td><td>0.860</td><td>0.825</td><td>0.811</td></tr></tbody></table></div><p>(2)The design of specialized experts<br>Our SAE module is the first to convert the physiological insight about stage progression and heterogeneity into a structured modeling strategy. This is beneficial for sleep staging precisely because of the heterogeneity problem described above. Many previous works (including cVAN) observe that N1 performance is much lower than for other stages, but they typically ignore the cause. We, in contrast, start from the underlying reason and design the architecture accordingly. This is a key point of novelty relative to cVAN.</p><p>(3)The contrast of efficiency<br>We achieve state-of-the-art performance with fewer parameters and faster training and inference. S3Net is therefore a lightweight yet powerful model for the challenging task of sleep staging.</p><div class="table-container"><table><thead><tr><th>model</th><th>Params(Million)</th><th>GFlops</th><th>train(ms)</th><th>infer(ms)</th></tr></thead><tbody><tr><td>MVF</td><td>39.51</td><td>11.06</td><td></td><td></td></tr><tr><td>cVAN</td><td>7.58</td><td>0.47</td><td>157.01</td><td>66.72</td></tr><tr><td>S3Net</td><td>6.49</td><td>2.70</td><td>75.69</td><td>24.02</td></tr></tbody></table></div><p>The test is on NVIDIA RTX A6000 at batch size 32 (averaged over 1000 runs; MVF-SleepNet is not end-to-end, so we report only its params and GFLOPs)</p><ol><li><p>Given the relatively modest performance gains, how is the added architectural complexity of S³Net justified?<br>Sleep staging is a mature area; while absolute gains are modest, our results consistently exceed baselines across all datasets and metrics. Efficiency-wise, S3Net has fewer parameters than prior baselines and comparable compute, while training and inference are significantly faster. As summarized above: MVF 39.51M params, 11.06 GFLOPs; cVAN 7.58M params, 0.47 GFLOPs, 157.01 ms train, 66.72 ms infer; S3Net 6.49M params, 2.70 GFLOPs, 75.69 ms train, 24.02 ms infer.</p></li><li><p>What quantitative evidence demonstrates that t-ALN improves alignment quality and that SAE learns stage-specific knowledge?<br>For alignment quality and stage-specific learning: our heatmap and t-SNE analyses show that t-ALN preserves temporal context and yields clearer class separation than no-ALN; controlled ablations corroborate these benefits. For SAE, cross-gate t-SNE exhibits clear separation between the two experts with tight intra-class clusters, and the gate classification accuracy reaches 97.1%, indicating distinct stage-specific knowledge is learned.Taken together, these findings support that the gains are consistent and practically meaningful, and that the added modules deliver a favorable accuracy–efficiency trade-off with interpretable stage-aware behavior.</p></li><li><p>What theoretical, physiological, or statistical evidence supports the assumption that transitional sleep stages require distinct sub-models?</p></li></ol><p>Metzner et al. (2021) — In a quantitative EEG transition analysis, a dominant Wake→N1→N2 path was observed, with N1 frequently oscillating with N2; “the stage-to-stage probabilities describe a strong propensity for transitions from Wake to N1 and from there to N2.” By contrast, stages with deep slow-wave or rhythmic activity show greater persistence: N3 is characterized by highly synchronized slow waves, whereas REM exhibits sustained theta-band activity.</p><p>Claus Metzner, Achim Schilling, Maximilian Traxdorf, Holger Schulze, and Patrick Krauss. Sleep as a random walk: A super-statistical analysis of EEG data across sleep stages. Communications Biology, 4:1385, 2021. doi: 10.1038/s42003-021-02912-6. URL: <a target="_blank" rel="noopener" href="https://www.nature.com/articles/s42003-021-02912-6">https://www.nature.com/articles/s42003-021-02912-6</a>.</p><p>Wijesinghe &amp; Lima (2025) — This EEG study distinguishes “light” versus “deep” stages via temporal features. N1 acts as a transitional stage: “N1, serving as a transitional stage between wakefulness and deeper sleep, naturally exhibits unstable dynamics, which aligns with its low autocovariance values.” REM and N3 exhibit significantly longer temporal dependencies than lighter stages (N1 and wake), reflecting sustained oscillatory patterns (N3 slow waves, REM theta). N1 is also ambiguous in EEG and hard to classify; it “remains the least confidently predicted, often contributing to rejected epochs” due to its intermediate, mixed features.</p><p>Dhanushka Wijesinghe and Ivan T. Lima, Jr. A lightweight neural network based on memory and transition probability for accurate real-time sleep stage classification. Brain Sciences, 15(8):789, 2025. doi: 10.3390/brainsci15080789. URL: <a target="_blank" rel="noopener" href="https://www.mdpi.com/2076-3425/15/8/789">https://www.mdpi.com/2076-3425/15/8/789</a>.</p><p>EEG Normal Sleep (StatPearls, 2024) — “Stage N1: alpha rhythm is attenuated and replaced by low‑amplitude, mixed‑frequency activity; slow eye movements may be present; vertex sharp waves can appear.” “Stage N2: presence of sleep spindles or K‑complexes.”<br>These canonical definitions clarify that N1 carries wake‑like low‑amplitude mixed‑frequency activity while N2 is event‑defined, supporting that N1 lies on the transition between W and N2.</p><p>Reference: Adeel M. Khan, Ramola S. S., et al. EEG Normal Sleep. StatPearls, 2024. URL: <a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/books/NBK537023/">https://www.ncbi.nlm.nih.gov/books/NBK537023/</a></p><p>Physiological definitions and scoring rules — “When an arousal interrupts stage N2 sleep, score subsequent segments as stage N1 if the EEG exhibits low‑amplitude, mixed‑frequency activity … Continue to score stage N1 until there is evidence for another stage, usually stage N2 or stage R.” “Attenuation of alpha rhythm was determined to be the most valid electrophysiological marker of sleep onset.” “Signs of transition from wakefulness to sleep include alpha attenuation/slowing, vertex sharp waves, incipient sleep spindles, slow eye movements, and brief ‘microsleep’ periods of low‑amplitude 4–7 Hz mixed‑frequency EEG.”<br>These rules position N1 as the operational transition state bridging wakefulness and stable N2/R, consistent with its mixed features and short persistence.</p><p>Reference: American Academy of Sleep Medicine. The AASM Manual for the Scoring of Sleep and Associated Events (Version 2.1 update summary). 2017. URL: <a target="_blank" rel="noopener" href="https://aasm.org/wp-content/uploads/2017/11/Summary-of-Updates-in-v2.1-FINAL.pdf">https://aasm.org/wp-content/uploads/2017/11/Summary-of-Updates-in-v2.1-FINAL.pdf</a><br>Reference: Michael H. Silber, Sonia Ancoli‑Israel, Michael H. Bonnet, Sudhansu Chokroverty, Madeleine M. Grigg‑Damberger, Max Hirshkowitz, Sheldon Kapen, Sharon A. Keenan, Meir H. Kryger, Thomas Penzel, Mark R. Pressman, and Conrad Iber. The visual scoring of sleep in adults. Journal of Clinical Sleep Medicine, 3(2):121–131, 2007. doi: 10.5664/jcsm.26814. URL: <a target="_blank" rel="noopener" href="https://jcsm.aasm.org/doi/10.5664/jcsm.26814">https://jcsm.aasm.org/doi/10.5664/jcsm.26814</a><br>Reference: Jeffrey Zimmerman. Stability versus transitional changes in the EEG: From sleep to wakefulness. Journal of Clinical Sleep Medicine, 11(9):1067–1075, 2015. doi: 10.5664/jcsm.4616. URL: <a target="_blank" rel="noopener" href="https://jcsm.aasm.org/doi/10.5664/jcsm.4616">https://jcsm.aasm.org/doi/10.5664/jcsm.4616</a></p><p>Confusion entropy (higher = more ambiguity) and inter-class variance (higher = greater deviation of transition behavior from the overall mean) by age group and overall dataset:</p><p>18–40 years — Confusion entropy (per class)<br>| W | N1 | N2 | N3 | REM |<br>| —- | —- | —- | —- | —- |<br>| 0.587743 | 1.504805 | 0.699646 | 0.517564 | 0.436499 |</p><p>Inter-class variance (per class)<br>| W | N1 | N2 | N3 | REM |<br>| —- | —- | —- | —- | —- |<br>| 0.114578 | 0.062937 | 0.009738 | 0.110816 | 0.030494 |</p><p>41–65 years — Confusion entropy (per class)<br>| W | N1 | N2 | N3 | REM |<br>| —- | —- | —- | —- | —- |<br>| 0.483451 | 1.466995 | 0.754874 | 0.598358 | 0.511859 |</p><p>Inter-class variance (per class)<br>| W | N1 | N2 | N3 | REM |<br>| —- | —- | —- | —- | —- |<br>| 0.14161 | 0.057958 | 0.012551 | 0.103621 | 0.021252 |</p><p>65+ years — Confusion entropy (per class)<br>| W | N1 | N2 | N3 | REM |<br>| —- | —- | —- | —- | —- |<br>| 0.365262 | 1.287355 | 0.727059 | 0.805349 | 0.471778 |</p><p>Inter-class variance (per class)<br>| W | N1 | N2 | N3 | REM |<br>| —- | —- | —- | —- | —- |<br>| 0.18156 | 0.047594 | 0.012029 | 0.136241 | 0.023169 |</p><p>All subjects — Confusion entropy (per class)<br>| W | N1 | N2 | N3 | REM |<br>| —- | —- | —- | —- | —- |<br>| 0.444948 | 1.404988 | 0.729318 | 0.624516 | 0.474131 |</p><p>Inter-class variance (per class)<br>| W | N1 | N2 | N3 | REM |<br>| —- | —- | —- | —- | —- |<br>| 0.141057 | 0.05344 | 0.010932 | 0.117301 | 0.034087 |</p><p>Confusion entropy captures ambiguity (higher means more confusion), while inter-class variance measures how strongly a class’s transition behavior departs from the overall mean. Wake (W) shows the highest inter-class variance, indicating the most distinctive transition pattern relative to the five‑class average—i.e., strong class discriminability. Combined with our transition matrix and the physiology of sleep—which progresses gradually W → N1 → N2 → N3 → REM—this provides a principled rationale for grouping W/N1/N2 as transitional and N3/REM as stable. The matrix further reveals large bidirectional transitions between W-N1 and N1-N2, while between N3 and N2 only the N3→N2 transition is slightly elevated. Under AASM scoring rules, N1 functions as an operational transition state and is only re‑labeled when clear evidence for another stage emerges; accordingly, N1 contains many features resembling both W and N2. These observations justify the proposed partition.</p><ol><li>Architectural differences and direct replacement/ablation<ul><li>Core mechanism: We reconstruct the time–frequency feature map into a native sequence of query tokens; these tokens then follow the same processing path as the temporal branch and finally enter the core windowed attention. This is not a simple projection or reshaping. Two architectural details are as follows.</li><li>t-ALN pipeline: square–energy normalization → dual weighting along channel and frequency → re-ordering → positional encoding → attention. This preserves full spectral detail and temporal order for faithful time–frequency interaction.</li><li>cVAN-style alignment: max-pooling compresses spectral features to scalars and later broadcasts them to token vectors, which loses fine spectral information and does not selectively retain features or guarantee temporal coherence.</li></ul></li></ol><div class="table-container"><table><thead><tr><th>version</th><th>ACC</th><th>F1</th><th>Kappa</th></tr></thead><tbody><tr><td>cVAN-like-ALN</td><td>0.858</td><td>0.820</td><td>0.812</td></tr><tr><td>pooling-ALN</td><td>0.861</td><td>0.831</td><td>0.816</td></tr><tr><td>flat-ALN</td><td>0.847</td><td>0.813</td><td>0.790</td></tr><tr><td>S3Net</td><td>0.866</td><td>0.855</td><td>0.827</td></tr></tbody></table></div><ul><li>Controlled experiments (same backbone, matched token shapes, identical training schedule and dataset splits):<ul><li>cVAN-like-ALN: replace our t-ALN with cVAN-style compress-to-scalar (max-pooling) followed by broadcast back to token vectors; we keep compress/broadcast to ensure downstream shapes match. ACC/F1/Kappa = 0.858/0.820/0.812 (Δ vs Full S3Net: −0.008/−0.035/−0.015).</li><li>Additional experiment 1 : keep the spectral-token reconstruction pipeline unchanged, but replace softmax-based dual weighting with learnable pooling; ACC/F1/Kappa = 0.861/0.831/0.816 (Δ: −0.005/−0.024/−0.011). Softmax acts as attention normalization along frequency/channel and provides better interpretability and empirical performance than pooling.</li><li>Additional experiment 2 : directly flatten the spectral map and add strong positional encoding, removing the dual weighting; ACC/F1/Kappa = 0.847/0.813/0.790 (Δ: −0.019/−0.042/−0.037). Flattening breaks temporal locality, making frequency-domain features nearly ineffective; positional encoding cannot recover the lost structure.</li><li>Full S3Net: energy normalization → dual channel–frequency weighting → re-ordering → positional encoding → windowed attention; ACC/F1/Kappa = 0.866/0.855/0.827.</li></ul></li><li>Under matched settings, all replacements lead to consistent drops, confirming that the gains come from detail-preserving spectral-token reconstruction and stage-aware gating—rather than parameter count.</li></ul><ol><li><p>Evidence of improved time–frequency alignment<br>We provide two visual analyses in the main paper. First, t-ALN vs no-ALN heatmaps: with t-ALN, temporal context is clearly preserved and class-wise patterns are distinct; with no-ALN, features appear disordered and behave like noise when interacting with the temporal branch, making effective learning difficult. The disruption of originally ordered features is illustrated in the introduction.<br>Second, t-SNE and feature views under three settings: (a) No Cross-Layer Interaction (no fusion between X1Q,X2Q and X1T,X2T), (b) No-ALN, (c) Full S3Net. After introducing t-ALN, inter-class margins widen and intra-class cohesion tightens; in X3E and X3Q under (c), class separability becomes clearer, evidencing the improvement brought by t-ALN.</p></li><li><p>Accuracy–efficiency trade-off?<br>Sleep staging is a mature field; achieving stable, cross-dataset gains of 1–3 points constitutes an effective improvement on high baselines and translates into a notable error reduction at high accuracy, especially for N1/N2.</p><ul><li>Efficiency: the added modules (dual experts, cross-gate) keep the model within a lightweight budget—fewer parameters than prior architectures, GFLOPs in the same order, and competitive measured latency under identical hardware and batch settings. MVF-SleepNet is not end-to-end, so we report only its parameters and GFLOPs.</li><li>We measured training and inference on NVIDIA RTX A6000 with batch size 32 over 1000 runs and report averaged times (see table), yielding a favorable accuracy–efficiency balance: better accuracy with lower runtime and smaller model size.</li></ul></li></ol><div class="table-container"><table><thead><tr><th>model</th><th>Params(Million)</th><th>GFlops</th><th>train(ms)</th><th>infer(ms)</th></tr></thead><tbody><tr><td>MVF</td><td>39.51</td><td>11.06</td><td></td><td></td></tr><tr><td>cVAN</td><td>7.58</td><td>0.47</td><td>157.01</td><td>66.72</td></tr><tr><td>S3Net</td><td>6.49</td><td>2.70</td><td>75.69</td><td>24.02</td></tr></tbody></table></div><p>We appreciate the constructive questions and have aligned our response to the reviewer’s ordering.</p><p>To Reviewer 1ogm:</p><p>We sincerely appreciate your careful and insightful assessment, which directly engages with our methodological rigor and empirical findings and has helped us refine the clarity and presentation of the work.</p><ol><li>Why is the specific stage partition (W/N1/N2 vs N3/REM) optimal?</li></ol><p>We further evaluated several alternative stage partitions. The following experiments demonstrate that our proposed partition (hard-to-separate: W, N1, N2; easy-to-separate: N3, REM) achieves the best overall performance, while also being consistent with physiological knowledge about sleep-stage progression.</p><div class="table-container"><table><thead><tr><th>Group</th><th>ACC</th><th>F1</th><th>Kappa</th><th>W</th><th>N1</th><th>N2</th><th>N3</th><th>REM</th></tr></thead><tbody><tr><td>W \ N1,N2 \ N3,REM</td><td>0.858</td><td>0.834</td><td>0.809</td><td>0.921</td><td>0.598</td><td>0.887</td><td>0.916</td><td>0.848</td></tr><tr><td>W,N1 \ N2,N3,REM</td><td>0.858</td><td>0.819</td><td>0.809</td><td>0.905</td><td>0.508</td><td>0.889</td><td>0.924</td><td>0.868</td></tr><tr><td>W,N1,N2,N3 \ REM</td><td>0.854</td><td>0.821</td><td>0.803</td><td>0.908</td><td>0.577</td><td>0.893</td><td>0.921</td><td>0.804</td></tr><tr><td>W,N2,N3,REM \ N1</td><td>0.849</td><td>0.812</td><td>0.796</td><td>0.885</td><td>0.508</td><td>0.890</td><td>0.918</td><td>0.860</td></tr><tr><td>W \ N1,N2,N3 \ REM</td><td>0.852</td><td>0.822</td><td>0.801</td><td>0.892</td><td>0.544</td><td>0.881</td><td>0.916</td><td>0.876</td></tr><tr><td>W,REM \ N1,N2,N3</td><td>0.860</td><td>0.842</td><td>0.818</td><td>0.920</td><td>0.642</td><td>0.859</td><td>0.924</td><td>0.880</td></tr></tbody></table></div><ol><li>What happens if model learn the grouping in an unsupervised manner?<br>We investigated this scenario in our ablation study (Variant M5: removing the auxiliary loss and letting the model learn the grouping implicitly). In this case, accuracy, F1-score, and κ decrease from 0.866, 0.855, and 0.827 (full setting) to 0.852, 0.838, and 0.810, respectively. Figure 7 further supports this finding: the visualization of the auxiliary loss shows that 97.1% of samples are grouped correctly, confirming that the auxiliary loss effectively guides the experts and improves the model’s efficiency and stability.</li></ol><ol><li>Why the auxiliary loss weight is set to 1?</li></ol><p>Our task is formulated as a multi-objective optimization problem. The auxiliary loss is essential to encourage each expert to focus on its designated stages, and it complements the main prediction loss, which determines the final stage labels. As shown in Figure 8, we validated this by sweeping the auxiliary loss weight α ∈ {0, 0.5, 1, 1.5, 2}. We found that α = 1 consistently yields the best F1 performance. Both larger and smaller values of α lead to worse results than α = 1, which empirically justifies our choice.</p><ol><li>Does T-ALN have an irreplaceable role?</li></ol><p>“Transformers routinely handle flattened image patches with learned positional embeddings” mainly addresses the ordering of tokens, rather than the problem of multi-scale temporal–frequency alignment. Positional embeddings restore absolute order, but they can still disturb the relative temporal synchronization across different time–frequency scales.</p><p>In contrast, our method converts the time–frequency image into a sequence in a way that strictly preserves the original temporal order, thereby maintaining temporal alignment across strides. The transformer layer is then applied on top of this sequence with positional embeddings, so the resulting representation jointly encodes both the original temporal order and the positional information. This design further supports the fundamental effectiveness of T-ALN in handling time–frequency alignment, beyond what standard positional embeddings provide.</p><p>To test whether T-ALN is truly necessary, we conducted ablation experiments in which: (i) T-ALN is replaced by cVAN’s alignment, (ii) a simple pooling operation replaces our “Representation Enhancement” component, (iii) features are completely flattened and equipped with a stronger positional embedding (ROPE), and (iv) T-ALN is used without “Representation Enhancement.” The results are as follows, showing that our alignment strategy with T-ALN, especially with Representation Enhancement, consistently offers advantages over alternative alignment schemes, highlighting its critical role in the model.</p><div class="table-container"><table><thead><tr><th>version</th><th>ACC</th><th>F1</th><th>Kappa</th></tr></thead><tbody><tr><td>cVAN-ALN</td><td>0.858</td><td>0.820</td><td>0.812</td></tr><tr><td>pooling-ALN</td><td>0.861</td><td>0.831</td><td>0.816</td></tr><tr><td>flat-ALN</td><td>0.847</td><td>0.813</td><td>0.790</td></tr><tr><td>no-weight-ALN</td><td>0.860</td><td>0.825</td><td>0.811</td></tr></tbody></table></div><ol><li>Why use element by element square as energy representation?</li></ol><p>Using the squared amplitude as a proxy for energy is standard practice in signal processing and EEG feature extraction, where time-domain or time–frequency “energy features” are routinely computed as squared samples or squared coefficients (see, e.g., <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/engineering/short-time-energy">https://www.sciencedirect.com/topics/engineering/short-time-energy</a> and <a target="_blank" rel="noopener" href="https://www.riverpublishers.com/pdf/ebook/chapter/RP_9788770040723C171.pdf">https://www.riverpublishers.com/pdf/ebook/chapter/RP_9788770040723C171.pdf</a>).</p><p>Our element-wise squaring operation is a stable, parameter-free way to emphasize high-amplitude regions, which is beneficial for cross-resolution alignment. Concretely, we first compute (X^2) at each time point as a local energy measure; this is then aggregated across channels to form a nonlinear representation of local energy sensitivity. This energy-enhanced representation is fed into the subsequent alignment layer, helping it decide how to capture and align salient time–frequency patterns.</p><ol><li>Does the hard/easy stage partition hold across different populations?</li></ol><p>In our experiments, ISRUC-S1 consists entirely of subjects with disordered sleep, whereas ISRUC-S3 and Sleep-EDF-153 contain only healthy subjects. Across all three datasets, the observed transition patterns are consistent with our hard/easy stage grouping, as illustrated in Figure 1(a) of the paper (sleep stage transition matrix).</p><p>We further stratified subjects by age and visualized the corresponding sleep stage transition matrices. All age groups exhibit the same characteristic pattern, supporting the validity of our hard/easy stage partition across different populations.</p><div style="display:flex;gap:12px;align-items:flex-start;margin:8px 0"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/transition_probs_18_40.png" style="width:33%;height:auto;object-fit:contain"> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/transition_probs_41_65.png" style="width:33%;height:auto;object-fit:contain"> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/transition_probs_gt_65.png" style="width:33%;height:auto;object-fit:contain"></div><ol><li>The computational cost analysis compared to other model</li></ol><p>We complement the comparison to other models (MVF-SleepNet and cVAN). Our model achieve a state-of-the-art performance and S3Net is therefore a lightweight yet powerful model for the challenging task of sleep staging without adding more complexity.</p><div class="table-container"><table><thead><tr><th>model</th><th>Params(Million)</th><th>GFlops</th><th>train(ms)</th><th>infer(ms)</th></tr></thead><tbody><tr><td>MVF</td><td>39.51</td><td>11.06</td><td></td><td></td></tr><tr><td>cVAN</td><td>7.58</td><td>0.47</td><td>157.01</td><td>66.72</td></tr><tr><td>S3Net</td><td>6.49</td><td>2.70</td><td>75.69</td><td>24.02</td></tr></tbody></table></div><ol><li>The confusion about channels using</li></ol><p>The task itself requires complex signals or multidimensional information, so using 10 channels is a very natural choice. The 10 channels are a thorough exploration of the signal dimension, and these channels can provide more spatial information, which helps improve the accuracy of the model. The setting of using 10 channels is widely adopted in many related fields, with experimental reproducibility and consistency with other similar works. For example, many baseline methods(cVAN and MVF-SleepNet) for electroencephalography (EEG) or other biological signal analysis typically use multiple channels to capture more comprehensive features.</p><p>Choosing a baseline with fewer channels may not necessarily be fair If a model with fewer channels is used, its signal information will decrease, which may lead to a decrease in model performance. Therefore, comparing baseline methods under these conditions may not fully reflect the performance of the model (S ³ Net), as it has more contextual understanding when using more signal information. And if multi-channel is used, some models may actually experience performance degradation because it is difficult to handle too much information, which actually reflects the powerful performance of our model.</p><p>We further investigate our model when missing-modality tests, which also test in a less channels, following is our result:</p><p>| EEG sinlge |<br>| Method | ACC | F1 | kappa | Wake | N1 | N2 | N3 | REM |<br>| —- | —- | —- | —- | —- | —- | —- | —- | —- |<br>| MERL (ICML 2024) | 0.7467 | 0.7295 | 0.6758 | 0.8524 | 0.5212 | 0.7328 | 0.8603 | 0.6808 |<br>| SleepSMC (ICLR 2025) | 0.7646 | 0.7397 | 0.6969 | 0.8882 | 0.5069 | 0.7467 | 0.8636 | 0.6932 |<br>| S3Net | 0.8454 | 0.8303 | 0.8010 | 0.9124 | 0.6406 | 0.8456 | 0.9141 | 0.8309 |</p><p>| EOG sinlge |<br>| Method | ACC | F1 | kappa | Wake | N1 | N2 | N3 | REM |<br>| —- | —- | —- | —- | —- | —- | —- | —- | —- |<br>| MERL (ICML 2024) | 0.6976 | 0.6741 | 0.6132 | 0.7996 | 0.3912 | 0.6808 | 0.8351 | 0.6640 |<br>| SleepSMC (ICLR 2025) | 0.7646 | 0.7168 | 0.6697 | 0.8386 | 0.4765 | 0.7360 | 0.8722 | 0.6607 |<br>| S3Net | 0.8198 | 0.8006 | 0.7676 | 0.8959 | 0.5686 | 0.8153 | 0.9067 | 0.8168 |</p><p>| EMG sinlge |<br>| Method | ACC | F1 | kappa | Wake | N1 | N2 | N3 | REM |<br>| —- | —- | —- | —- | —- | —- | —- | —- | —- |<br>| MERL (ICML 2024) | 0.3981 | 0.3907 | 0.2348 | 0.4875 | 0.2077 | 0.3879 | 0.4008 | 0.4696 |<br>| SleepSMC (ICLR 2025) | 0.4384 | 0.4075 | 0.2693 | 0.5868 | 0.1281 | 0.4404 | 0.4301 | 0.4523 |<br>| S3Net | 0.5721 | 0.5406 | 0.4439 | 0.7150 | 0.2170 | 0.5520 | 0.5820 | 0.6369 |</p><p>These results show that S3Net maintains strong performance even when restricted to a single modality, further supporting the effectiveness of our design.</p><ol><li><p>Limited novelty of work？</p></li><li><p>Training dynamics:<br>The training curves clearly show that the model converges smoothly: training loss decreases monotonically and training accuracy continually improves without oscillation, indicating stable optimization. Validation loss and accuracy reach their optimal point around epoch ~15, after which mild fluctuations appear—this is expected as the model begins to overfit and the validation set is smaller, leading to higher variance. Importantly, these fluctuations occur only after the model has already converged to its best generalization performance. Therefore, the training process is stable, well-behaved, and convergent. We select the checkpoint at the best validation epoch to ensure the reported results correspond to the optimal and stable model.</p><div style="display:flex;gap:12px;align-items:flex-start;margin:8px 0"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/loss_curve.png" style="width:50%;height:auto;object-fit:contain"> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/acc_curve.png" style="width:50%;height:auto;object-fit:contain"></div></li><li><p>What happens if you let the model learn the grouping in an unsupervised manner?<br>We conducted an ablation study where the grouping module was trained in a fully unsupervised manner, i.e., without the auxiliary loss. The results show a clear performance degradation: overall accuracy drops from 0.866 to 0.852, F1 from 0.855 to 0.838, and Cohen’s kappa from 0.827 to 0.810. More importantly, the accuracy of the expert-category prediction falls sharply to 63.2%, compared with 97.1% when the auxiliary supervision is used. This substantial gap indicates that the model struggles to discover meaningful and consistent group structures on its own, and that the auxiliary loss plays a crucial role in guiding the grouping module toward semantically coherent clusters.</p></li></ol><p>To reviewer MXY5:</p><p>We sincerely thank the reviewers for their thoughtful evaluations and constructive feedback. We have addressed each point in the revision and responses, adding targeted ablations (including expert-count analyses and alternative weighting schemes), controlled comparisons, visual analyses, and generalization checks. Your comments have substantially improved the clarity, rigor, and robustness of the manuscript.</p><ol><li>Other experiment about t-ALN AND SAE<br>In fact, we have ablation experiments for the number of experts. See our superparametric experiments for details. In addition, we have prepared the following experiments on alternative attentional weighting schemes or other ablation experiments.<br>Controlled experiments (same backbone, matched token shapes, identical training schedule and dataset splits):<br>cVAN-like-ALN: replace our t-ALN with cVAN-style compress-to-scalar (max-pooling) followed by broadcast back to token vectors; we keep compress/broadcast to ensure downstream shapes match. ACC/F1/Kappa = 0.858/0.820/0.812 (Δ vs Full S3Net: −0.008/−0.035/−0.015).<br>Additional experiment 1 : keep the spectral-token reconstruction pipeline unchanged, but replace softmax-based dual weighting with learnable pooling; ACC/F1/Kappa = 0.861/0.831/0.816 (Δ: −0.005/−0.024/−0.011). Softmax acts as attention normalization along frequency/channel and provides better interpretability and empirical performance than pooling.<br>Additional experiment 2 : directly flatten the spectral map and add strong positional encoding, removing the dual weighting; ACC/F1/Kappa = 0.847/0.813/0.790 (Δ: −0.019/−0.042/−0.037). Flattening breaks temporal locality, making frequency-domain features nearly ineffective; positional encoding cannot recover the lost structure.<br>Full S3Net: energy normalization → dual channel–frequency weighting → re-ordering → positional encoding → windowed attention; ACC/F1/Kappa = 0.866/0.855/0.827.<br>Under matched settings, all replacements lead to consistent drops, confirming that the gains come from detail-preserving spectral-token reconstruction and stage-aware gating—rather than parameter count.</li></ol><div class="table-container"><table><thead><tr><th>version</th><th>ACC</th><th>F1</th><th>Kappa</th></tr></thead><tbody><tr><td>cVAN-like-ALN</td><td>0.858</td><td>0.820</td><td>0.812</td></tr><tr><td>pooling-ALN</td><td>0.861</td><td>0.831</td><td>0.816</td></tr><tr><td>flat-ALN</td><td>0.847</td><td>0.813</td><td>0.790</td></tr><tr><td>S3Net</td><td>0.866</td><td>0.855</td><td>0.827</td></tr></tbody></table></div><ol><li>How does the proposed method demonstrate generalization beyond within-domain EEG-based PSG datasets, and what evidence supports its robustness under cross-dataset or missing-modality conditions?</li></ol><p>Yes. To assess generalization, we additionally evaluated S³Net on a single-lead PPG dataset (WESAD, 4-class). Since SAE’s stage-specific experts are sleep-oriented, we removed the expert modules, retained the cross-gating mechanism, and replaced the output layer with a generic classification head. Under this setting, S³Net achieved 0.853 ACC, 0.911 AUROC, 0.833 AUPRC, 0.819 F1, and 0.787 Kappa—substantially outperforming recent baselines. These results support the broader utility and adaptability of the proposed architecture for physiological time-series classification beyond EEG sleep staging.</p><div class="table-container"><table><thead><tr><th>model</th><th>ACC</th><th>AUROC</th><th>AUPRC</th><th>F1</th><th>Kappa</th></tr></thead><tbody><tr><td>REBAR (ICLR 2024)</td><td>0.418</td><td>0.698</td><td>0.446</td><td></td><td></td></tr><tr><td>cVAN</td><td>0.6914</td><td>0.8550</td><td>0.7104</td><td>0.6641</td><td>0.5716</td></tr><tr><td>Resnet</td><td>0.713</td><td>0.877</td><td>0.746</td><td>0.682</td><td>0.593</td></tr><tr><td>HuBERT (NeurIPS 2025)</td><td>0.775</td><td>0.82</td><td></td><td></td><td></td></tr><tr><td>S3Net</td><td>0.853</td><td>0.911</td><td>0.833</td><td>0.819</td><td>0.787</td></tr></tbody></table></div><p>To further evaluate robustness and generalization under missing-modality conditions, we conducted single-modality experiments on EEG, EOG, and EMG signals. Across all three settings, S³Net consistently outperformed strong recent baselines (MERL, SleepSMC), demonstrating that the proposed alignment and cross-gating mechanisms remain effective even when only one modality is available. Notably, S³Net improved accuracy, F1, and Cohen’s κ across all modalities, with particularly large gains in the more challenging EMG-only scenario. These results indicate that the architecture does not rely on multimodal redundancy and can robustly extract discriminative temporal–spectral cues from any single physiological channel.</p><p>| EEG sinlge |<br>| Method | ACC | F1 | kappa | Wake | N1 | N2 | N3 | REM |<br>| —- | —- | —- | —- | —- | —- | —- | —- | —- |<br>| MERL (ICML 2024) | 0.7467 | 0.7295 | 0.6758 | 0.8524 | 0.5212 | 0.7328 | 0.8603 | 0.6808 |<br>| SleepSMC (ICLR 2025) | 0.7646 | 0.7397 | 0.6969 | 0.8882 | 0.5069 | 0.7467 | 0.8636 | 0.6932 |<br>| S3Net | 0.8454 | 0.8303 | 0.8010 | 0.9124 | 0.6406 | 0.8456 | 0.9141 | 0.8309 |</p><p>| EOG sinlge |<br>| Method | ACC | F1 | kappa | Wake | N1 | N2 | N3 | REM |<br>| —- | —- | —- | —- | —- | —- | —- | —- | —- |<br>| MERL (ICML 2024) | 0.6976 | 0.6741 | 0.6132 | 0.7996 | 0.3912 | 0.6808 | 0.8351 | 0.6640 |<br>| SleepSMC (ICLR 2025) | 0.7646 | 0.7168 | 0.6697 | 0.8386 | 0.4765 | 0.7360 | 0.8722 | 0.6607 |<br>| S3Net | 0.8198 | 0.8006 | 0.7676 | 0.8959 | 0.5686 | 0.8153 | 0.9067 | 0.8168 |</p><p>| EMG sinlge |<br>| Method | ACC | F1 | kappa | Wake | N1 | N2 | N3 | REM |<br>| —- | —- | —- | —- | —- | —- | —- | —- | —- |<br>| MERL (ICML 2024) | 0.3981 | 0.3907 | 0.2348 | 0.4875 | 0.2077 | 0.3879 | 0.4008 | 0.4696 |<br>| SleepSMC (ICLR 2025) | 0.4384 | 0.4075 | 0.2693 | 0.5868 | 0.1281 | 0.4404 | 0.4301 | 0.4523 |<br>| S3Net | 0.5721 | 0.5406 | 0.4439 | 0.7150 | 0.2170 | 0.5520 | 0.5820 | 0.6369 |</p></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E9%A1%B9%E7%9B%AE/">项目</a><a class="post-meta__tags" href="/tags/python/">python</a></div><div class="post-share"><div class="social-share" data-image="/img/head.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="next-post pull-full" href="/2025/11/17/%E9%A1%B9%E7%9B%AE%E6%B1%87%E6%8A%A58.0/" title="项目汇报8.0"><div class="cover" style="background:var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">项目汇报8.0</div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a href="/2025/10/12/%E5%96%89%E9%95%9C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" title="喉镜数据分析"><div class="cover" style="background:var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-12</div><div class="title">喉镜数据分析</div></div></a><a href="/2025/11/02/%E5%96%89%E9%95%9C%E7%BB%93%E6%9E%84%E8%A7%82%E6%B5%8B%E6%B1%87%E6%8A%A5/" title="喉镜结构观测汇报"><div class="cover" style="background:var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-02</div><div class="title">喉镜结构观测汇报</div></div></a><a href="/2025/06/19/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E7%9D%A1%E7%9C%A0%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E9%93%BE/" title="多模态智能体的睡眠分析平台"><div class="cover" style="background:var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-19</div><div class="title">多模态智能体的睡眠分析平台</div></div></a><a href="/2025/06/28/%E9%A1%B9%E7%9B%AE%E5%91%A8%E8%AE%B0/" title="项目周记录"><div class="cover" style="background:var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-28</div><div class="title">项目周记录</div></div></a><a href="/2025/06/07/%E9%A1%B9%E7%9B%AE%E6%B1%87%E6%8A%A52.0/" title="项目汇报2.0"><div class="cover" style="background:var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-07</div><div class="title">项目汇报2.0</div></div></a><a href="/2025/07/14/%E9%A1%B9%E7%9B%AE%E6%B1%87%E6%8A%A53.0/" title="项目汇报3.0"><div class="cover" style="background:var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-14</div><div class="title">项目汇报3.0</div></div></a></div></div><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/head.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">xxxkkw</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">34</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxkkw"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">我的小站</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/20/review/" title="review">review</a><time datetime="2025-11-20T06:00:00.000Z" title="发表于 2025-11-20 14:00:00">2025-11-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/17/%E9%A1%B9%E7%9B%AE%E6%B1%87%E6%8A%A58.0/" title="项目汇报8.0">项目汇报8.0</a><time datetime="2025-11-17T07:00:00.000Z" title="发表于 2025-11-17 15:00:00">2025-11-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/07/%E5%9B%A2%E9%98%9F%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/" title="团队开发入门指南">团队开发入门指南</a><time datetime="2025-11-07T07:30:00.000Z" title="发表于 2025-11-07 15:30:00">2025-11-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/04/%E6%A0%87%E7%AD%BE%E5%B7%A5%E5%85%B7%E6%95%99%E5%AD%A6%E5%90%91/" title="标签工具教学向">标签工具教学向</a><time datetime="2025-11-04T10:30:00.000Z" title="发表于 2025-11-04 18:30:00">2025-11-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/02/%E5%96%89%E9%95%9C%E7%BB%93%E6%9E%84%E8%A7%82%E6%B5%8B%E6%B1%87%E6%8A%A5/" title="喉镜结构观测汇报">喉镜结构观测汇报</a><time datetime="2025-11-02T14:00:00.000Z" title="发表于 2025-11-02 22:00:00">2025-11-02</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By xxxkkw</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(()=>{const t=()=>{new Valine(Object.assign({el:"#vcomment",appId:"NuwmhVS9BmgOpiVJWroZWSWW-gzGzoHsz",appKey:"NJWb1pCdV9rcU5odlHizyxsJ",avatar:"monsterid",serverURLs:"",emojiMaps:"",path:window.location.pathname,visitor:!1},null))},e=async()=>{"function"==typeof Valine||await btf.getScript("https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"),t()};setTimeout(e,0)})()</script></div></div></body></html>